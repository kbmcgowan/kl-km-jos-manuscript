<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.28">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Kyler Laycock">
<meta name="author" content="&nbsp;Kevin B McGowan">
<meta name="dcterms.date" content="2024-10-18">
<meta name="description" content="REMOVING THE DISGUISE: THE MATCHED GUISE TECHNIQUE AND LISTENER AWARENESS">

<title>Removing the disguise: the matched guise technique and listener awareness</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-75d73ece3e5fc6e04b504c8afc5573bd.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-8c563cdaaaea7e8b72d6a2a6edfa4172.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/critic/critic.min.js"></script>
<link href="site_libs/quarto-contrib/critic/critic.css" rel="stylesheet">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="_extensions/wjschne/apaquarto/apa.css">
<meta name="citation_title" content="Removing the disguise: the matched guise technique and listener awareness">
<meta name="citation_abstract" content="Sociophonetic perception is often studied using versions of the matched guise technique. Linguists using this technique appear united in the methodological assumptions that participants believe the manipulation and that this belief influences perception below the level of introspective awareness. We report an audiovisual matched guise experiment with a novel &amp;amp;#039;unhidden' instruction condition. The basic task is a replication of the Strand effect [@strandJohnson1996; @strand1999]. Participants in the 'unhidden' condition were instructed that the man or woman in the photo did not represent the voice they were listening to. Participants in both guises exhibited the Strand effect to nearly numerically identical extents. This result suggests that participants need not believe a link exists between a voice and a purported social category for visually-cued social information to influence segmental perception. We explore the implications of this result for the MGT and for theories of social awareness and speech perception more broadly.
">
<meta name="citation_keywords" content="">
<meta name="citation_author" content="Kyler Laycock">
<meta name="citation_author" content="&amp;amp;nbsp;Kevin B McGowan">
<meta name="citation_publication_date" content="2024-10-18">
<meta name="citation_cover_date" content="2024-10-18">
<meta name="citation_year" content="2024">
<meta name="citation_online_date" content="2024-10-18">
<meta name="citation_language" content="en">
<meta name="citation_journal_title" content="Awareness and Control of Sociolinguistic Variation">
<meta name="citation_reference" content="citation_title=Properties of the sociolinguistic monitor;,citation_author=William Labov;,citation_author=Sharon Ash;,citation_author=Maya Ravindranath;,citation_author=Tracey Weldon;,citation_author=Maciej Baranowski;,citation_author=Naomi Nagy;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_issue=4;,citation_volume=15;,citation_journal_title=Journal of Sociolinguistics;">
<meta name="citation_reference" content="citation_title=Integrating speech information across talkers, gender, and sensory modality: Female faces and male voices in the McGurk effect;,citation_author=Kerry Green;,citation_author=Patricia Kuhl;,citation_author=Andrew Meltzoff;,citation_author=Erica Stevens;,citation_publication_date=1991;,citation_cover_date=1991;,citation_year=1991;,citation_fulltext_html_url=http://dx.doi.org/10.3758/BF03207536;,citation_issue=6;,citation_issn=1943-3921;,citation_volume=50;,citation_journal_title=Attention, Perception, &amp;amp;amp; Psychophysics;">
<meta name="citation_reference" content="citation_title=Congruence between ‘word age’and ‘voice age’facilitates lexical access;,citation_author=Abby Walker;,citation_author=Jen Hay;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_issue=1;,citation_volume=2;,citation_journal_title=Laboratory Phonology;,citation_publisher=Walter de Gruyter GmbH &amp;amp;amp; Co. KG;">
<meta name="citation_reference" content="citation_title=Speech perception as a talker-contingent process;,citation_author=Lynne C Nygaard;,citation_author=Mitchell S Sommers;,citation_author=David B Pisoni;,citation_publication_date=1994;,citation_cover_date=1994;,citation_year=1994;,citation_issue=1;,citation_volume=5;,citation_journal_title=Psychological Science;">
<meta name="citation_reference" content="citation_title=The social life of phonetics and phonology;,citation_author=Paul Foulkes;,citation_author=Gerard Docherty;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;,citation_volume=34;,citation_journal_title=Journal of Phonetics;">
<meta name="citation_reference" content="citation_title=Social salience and the sociolinguistic monitor: A case study of ING and TH-fronting in britain;,citation_author=E. Levon;,citation_author=S. Fox;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_fulltext_html_url=https://doi.org/10.1177/0075424214531487;,citation_issue=3;,citation_doi=10.1177/0075424214531487;,citation_volume=42;,citation_journal_title=Journal of English Linguistics;">
<meta name="citation_reference" content="citation_title=Folk linguistics and the perception of language variety;,citation_author=Dennis R Preston;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_volume=10;,citation_journal_title=Handbuch Sprache im Urteil der Öffentlichkeit;,citation_publisher=Walter de Gruyter GmbH &amp;amp;amp; Co KG;">
<meta name="citation_reference" content="citation_title=Three waves of variation study: The emergence of meaning in the study of sociolinguistic variation;,citation_author=Penelope Eckert;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_issue=1;,citation_volume=41;,citation_journal_title=Annual review of Anthropology;">
<meta name="citation_reference" content="citation_title=Phonetic trading relations and context effects: New experimental evidence for a speech mode of perception.;,citation_author=Bruno H Repp;,citation_publication_date=1982;,citation_cover_date=1982;,citation_year=1982;,citation_issue=1;,citation_volume=92;,citation_journal_title=Psychological bulletin;,citation_publisher=American Psychological Association;">
<meta name="citation_reference" content="citation_title=Embodied sociolinguistics;,citation_author=Mary Bucholtz;,citation_author=Kira Hall;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_issue=1;,citation_volume=1;,citation_journal_title=Sociolinguistics: theoretical debates;,citation_publisher=Cambridge University Press Cambridge;">
<meta name="citation_reference" content="citation_title=The frequency code underlies the sound-symbolic use of voice pitch;,citation_author=John J Ohala;,citation_publication_date=1994;,citation_cover_date=1994;,citation_year=1994;,citation_journal_title=Sound symbolism;,citation_publisher=Cambridge University Press;">
<meta name="citation_reference" content="citation_title=Cross-language use of pitch: An ethological view;,citation_author=John J Ohala;,citation_publication_date=1983;,citation_cover_date=1983;,citation_year=1983;,citation_issue=1;,citation_volume=40;,citation_journal_title=Phonetica;,citation_publisher=S. Karger AG;">
<meta name="citation_reference" content="citation_title=The emergence of the unmarked;,citation_author=Rusty Barrett;,citation_editor=L Zimman;,citation_editor=J Davis;,citation_editor=J Raclaw;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_inbook_title=Queer excursions: Retheorizing binaries in language, gender, and sexuality;">
<meta name="citation_reference" content="citation_title=From “sex differences” to gender variation in sociolinguistics;,citation_author=Mary Bucholtz;,citation_publication_date=2002;,citation_cover_date=2002;,citation_year=2002;,citation_issue=3;,citation_volume=8;,citation_journal_title=University of Pennsylvania Working Papers in Linguistics;">
<meta name="citation_reference" content="citation_title=From “sex differences” to gender variation in sociolinguistics;,citation_author=Mary Bucholtz;,citation_publication_date=2002;,citation_cover_date=2002;,citation_year=2002;,citation_issue=3;,citation_volume=8;,citation_journal_title=University of Pennsylvania Working Papers in Linguistics;">
<meta name="citation_reference" content="citation_title=Sociophonetics, Gender, and Sexuality;,citation_author=Robert J. Podesva;,citation_author=Sakiko Kajino;,citation_editor=Susan Ehrlich;,citation_editor=Miriam Meyerhoff;,citation_editor=Janet Holmes;,citation_publication_date=2014-03;,citation_cover_date=2014-03;,citation_year=2014;,citation_fulltext_html_url=https://onlinelibrary.wiley.com/doi/10.1002/9781118584248.ch5;,citation_doi=10.1002/9781118584248.ch5;,citation_isbn=978-1-118-58424-8 978-0-470-65642-6;,citation_inbook_title=The Handbook of Language, Gender, and Sexuality;">
<meta name="citation_reference" content="citation_title=Sociophonetics, Gender, and Sexuality;,citation_author=Robert J. Podesva;,citation_author=Sakiko Kajino;,citation_editor=Susan Ehrlich;,citation_editor=Miriam Meyerhoff;,citation_editor=Janet Holmes;,citation_publication_date=2014-03;,citation_cover_date=2014-03;,citation_year=2014;,citation_fulltext_html_url=https://onlinelibrary.wiley.com/doi/10.1002/9781118584248.ch5;,citation_doi=10.1002/9781118584248.ch5;,citation_isbn=978-1-118-58424-8 978-0-470-65642-6;,citation_inbook_title=The Handbook of Language, Gender, and Sexuality;">
<meta name="citation_reference" content="citation_title=Non-binary approaches to gender and sexuality;,citation_author=Penelope Eckert;,citation_author=Robert J Podesva;,citation_editor=Jo Angouri;,citation_editor=Judith Baxter;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_inbook_title=The routledge handbook of language, gender, and sexuality;">
<meta name="citation_reference" content="citation_title=Language and gender;,citation_author=Kira Hall;,citation_author=Rodrigo Borba;,citation_author=Mie Hiramoto;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=The international encyclopedia of linguistic anthropology;,citation_publisher=John Wiley &amp;amp;amp; Sons Hoboken;">
<meta name="citation_reference" content="citation_title=Uncovering the role of gender stereotypes in speech perception;,citation_author=Elizabeth A Strand;,citation_publication_date=1999;,citation_cover_date=1999;,citation_year=1999;,citation_issue=1;,citation_volume=18;,citation_journal_title=Journal of language and social psychology;,citation_publisher=Sage Publications Sage CA: Thousand Oaks, CA;">
<meta name="citation_reference" content="citation_title=Gradient and visual speaker normalization in the perception of fricatives.;,citation_author=Elizabeth A Strand;,citation_author=Keith Johnson;,citation_publication_date=1996;,citation_cover_date=1996;,citation_year=1996;,citation_conference_title=KONVENS;">
<meta name="citation_reference" content="citation_title=Acoustic theory of speech production;,citation_author=G. Fant;,citation_publication_date=1960;,citation_cover_date=1960;,citation_year=1960;">
<meta name="citation_reference" content="citation_title=The effect of geometry on source mechanisms of fricative consonants;,citation_author=Christine H Shadle;,citation_publication_date=1991;,citation_cover_date=1991;,citation_year=1991;,citation_issue=3-4;,citation_volume=19;,citation_journal_title=Journal of Phonetics;,citation_publisher=Elsevier;">
<meta name="citation_reference" content="citation_title=Subcategorical phonetic mismatches slow phonetic judgments;,citation_author=Douglas H Whalen;,citation_publication_date=1984;,citation_cover_date=1984;,citation_year=1984;,citation_volume=35;,citation_journal_title=Perception &amp;amp;amp; Psychophysics;,citation_publisher=Springer;">
<meta name="citation_reference" content="citation_title=Vocal tract normalization for /s/ and /š/;,citation_author=Janet May;,citation_publication_date=1976;,citation_cover_date=1976;,citation_year=1976;,citation_issue=SR-48;,citation_journal_title=Haskins Laboratories Status Report on Speech Research;">
<meta name="citation_reference" content="citation_title=The influence of actual and imputed talker gender on fricative perception, revisited (l);,citation_author=Benjamin Munson;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_issue=5;,citation_volume=130;,citation_journal_title=The Journal of the Acoustical Society of America;,citation_publisher=Acoustical Society of America;">
<meta name="citation_reference" content="citation_title=Influence of vocalic context on perception of the [ʃ]-[s] distinction;,citation_author=Virginia A Mann;,citation_author=Bruno H Repp;,citation_publication_date=1980;,citation_cover_date=1980;,citation_year=1980;,citation_issue=3;,citation_volume=28;,citation_journal_title=Perception &amp;amp;amp; Psychophysics;,citation_publisher=Springer;">
<meta name="citation_reference" content="citation_title=An ethological perspective on common cross-language utilization of F₀ of voice;,citation_author=John J Ohala;,citation_publication_date=1984;,citation_cover_date=1984;,citation_year=1984;,citation_issue=1;,citation_volume=41;,citation_journal_title=Phonetica;,citation_publisher=S. Karger AG Basel, Switzerland;">
<meta name="citation_reference" content="citation_title=On the influence of context upon perception of voiceless fricative consonants;,citation_author=Osamu Kunisaki;,citation_author=Hyroya Fujisaki;,citation_publication_date=1977;,citation_cover_date=1977;,citation_year=1977;,citation_volume=11;,citation_journal_title=Annual Bulletin;,citation_publisher=Research Institute of Logopedics; Phoniatrics Tokyo;">
<meta name="citation_reference" content="citation_title=Effects of vocalic formant transitions and vowel quality on the english [s]–[š] boundary;,citation_author=Douglas H Whalen;,citation_publication_date=1981;,citation_cover_date=1981;,citation_year=1981;,citation_issue=1;,citation_volume=69;,citation_journal_title=The Journal of the Acoustical Society of America;,citation_publisher=Acoustical Society of America;">
<meta name="citation_reference" content="citation_title=The time course of perception of coarticulation;,citation_author=Patrice Speeter Beddor;,citation_author=Kevin B McGowan;,citation_author=Julie E Boland;,citation_author=Andries W Coetzee;,citation_author=Anthony Brasher;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_issue=4;,citation_volume=133;,citation_journal_title=The Journal of the Acoustical Society of America;,citation_publisher=AIP Publishing;">
<meta name="citation_reference" content="citation_title=The time course of individuals’ perception of coarticulatory information is linked to their production: Implications for sound change;,citation_author=Patrice Speeter Beddor;,citation_author=Andries W Coetzee;,citation_author=Will Styler;,citation_author=Kevin B McGowan;,citation_author=Julie E Boland;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=4;,citation_volume=94;,citation_journal_title=Language;,citation_publisher=Linguistic Society of America;">
<meta name="citation_reference" content="citation_title=Hearing lips and seeing voices;,citation_author=Harry McGurk;,citation_author=John MacDonald;,citation_publication_date=1976;,citation_cover_date=1976;,citation_year=1976;,citation_volume=264;,citation_journal_title=Nature;">
<meta name="citation_reference" content="citation_title=Perception of the english/s/–//distinction relies on fricative noises and transitions, not on brief spectral slices;,citation_author=Douglas H Whalen;,citation_publication_date=1991;,citation_cover_date=1991;,citation_year=1991;,citation_issue=4;,citation_volume=90;,citation_journal_title=The Journal of the Acoustical Society of America;,citation_publisher=Acoustical Society of America;">
<meta name="citation_reference" content="citation_title=Sociophonetics, gender, and sexuality;,citation_author=Robert J Podesva;,citation_author=Sakiko Kajino;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_journal_title=The handbook of language, gender, and sexuality;,citation_publisher=Wiley Online Library;">
<meta name="citation_reference" content="citation_title=The association between/s/quality and perceived sexual orientation of men’s voices: Implicit and explicit measures;,citation_author=Sara Mack;,citation_author=Benjamin Munson;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_issue=1;,citation_volume=40;,citation_journal_title=Journal of Phonetics;">
<meta name="citation_reference" content="citation_title=From “gay lisp” to “fierce queen”: The sociophonetics of sexuality’s most iconic variable;,citation_author=J Calder;,citation_editor=Kira Hall;,citation_editor=Rusty Barrett;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_inbook_title=The oxford handbook of language and sexuality;">
<meta name="citation_reference" content="citation_title=Pharyngeal dimensions in healthy men and women;,citation_author=Mauro Miguel Daniel;,citation_author=Maria Cecı́lia Lorenzi;,citation_author=Claudia Costa Leite;,citation_author=Geraldo Lorenzi-Filho;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_issue=1;,citation_volume=62;,citation_journal_title=Clinics;,citation_publisher=Elsevier;">
<meta name="citation_reference" content="citation_title=Changes in nasal cavity dimensions in children and adults by gender and age;,citation_author=Bolesław K Samoliński;,citation_author=Antoni Grzanka;,citation_author=Tomasz Gotlib;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_issue=8;,citation_volume=117;,citation_journal_title=The Laryngoscope;,citation_publisher=Wiley Online Library;">
<meta name="citation_reference" content="citation_title=The acoustic bases for gender identification from children’s voices;,citation_author=Theodore L Perry;,citation_author=Ralph N Ohde;,citation_author=Daniel H Ashmead;,citation_publication_date=2001;,citation_cover_date=2001;,citation_year=2001;,citation_issue=6;,citation_volume=109;,citation_journal_title=The Journal of the Acoustical Society of America;,citation_publisher=Acoustical Society of America;">
<meta name="citation_reference" content="citation_title=Gradient perception of children’s productions of/s/and/$\theta$: A comparative study of rating methods;,citation_author=Sarah K Schellinger;,citation_author=Benjamin Munson;,citation_author=Jan Edwards;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_issue=1;,citation_volume=31;,citation_journal_title=Clinical Linguistics &amp;amp;amp; Phonetics;,citation_publisher=Taylor &amp;amp; Francis;">
<meta name="citation_reference" content="citation_title=Non-binary approaches to gender and sexuality;,citation_author=Penelope Eckert;,citation_author=Robert J Podesva;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=The Routledge handbook of language, gender, and sexuality;,citation_publisher=Routledge;">
<meta name="citation_reference" content="citation_title=Speaker normalization in speech perception;,citation_author=K. Johnson;,citation_editor=D. B. Pisoni;,citation_editor=R. Remez;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;,citation_inbook_title=The handbook of speech perception;">
<meta name="citation_reference" content="citation_title=Resonance in an exemplar-based lexicon: The emergence of social identity and phonology.;,citation_author=Keith Johnson;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;,citation_volume=34;,citation_journal_title=Journal of Phonetics;">
<meta name="citation_reference" content="citation_title=Variation and the indexical field 1;,citation_author=Penelope Eckert;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_issue=4;,citation_volume=12;,citation_journal_title=Journal of sociolinguistics;,citation_publisher=Wiley Online Library;">
<meta name="citation_reference" content="citation_title=Conceptualizing thai genderscapes: Transformation and continuity in the thai sex/gender system;,citation_author=Dredge Byung’chu Käng;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_inbook_title=Contemporary socio-cultural and political perspectives in thailand;">
<meta name="citation_reference" content="citation_title=Language, gender, and ideology in japanese professional matchmaking.;,citation_author=Erika Renée Alpert;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_dissertation_institution=University of Michigan, Department of Anthropology;">
<meta name="citation_reference" content="citation_title=Crosslinguistic perceptions of /s/ among english, french, and german listeners;,citation_author=Zac Boyd;,citation_author=Josef Fruehwald;,citation_author=Lauren Hall-Lew;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_issue=2;,citation_doi=10.1017/S0954394521000089;,citation_volume=33;,citation_journal_title=Language Variation and Change;,citation_publisher=Cambridge University Press;">
<meta name="citation_reference" content="citation_title=Listener perceptions of sociolinguistic variables: The case of (ING);,citation_author=Kathryn Campbell-Kibler;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;,citation_dissertation_institution=Stanford University;">
<meta name="citation_reference" content="citation_title=Accent,(ING), and the social logic of listener perceptions;,citation_author=Kathryn Campbell-Kibler;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_issue=1;,citation_volume=82;,citation_journal_title=American speech;,citation_publisher=Duke University Press;">
<meta name="citation_reference" content="citation_title=Gender as stylistic bricolage: Transmasculine voices and the relationship between fundamental frequency and/s;,citation_author=Lal Zimman;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_issue=3;,citation_volume=46;,citation_journal_title=Language in Society;,citation_publisher=Cambridge University Press;">
<meta name="citation_reference" content="citation_title=Situating experience in social meaning: Ethnography, experiments and exemplars in the enregisterment of istanbul greek;,citation_author=Matthew Hadodo;,citation_issue=1;,citation_volume=42;,citation_journal_title=Journal of Sociolinguistics;">
<meta name="citation_reference" content="citation_title=Verbal guise test: Problems and solutions;,citation_author=Ka Long Roy CHAN;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=Academia Letters;">
<meta name="citation_reference" content="citation_title=Evaluational reactions to spoken languages.;,citation_author=Wallace E Lambert;,citation_author=Richard C Hodgson;,citation_author=Robert C Gardner;,citation_author=Samuel Fillenbaum;,citation_publication_date=1960;,citation_cover_date=1960;,citation_year=1960;,citation_issue=1;,citation_volume=60;,citation_journal_title=The journal of abnormal and social psychology;,citation_publisher=American Psychological Association;">
<meta name="citation_reference" content="citation_title=Nonlanguage factors affecting undergraduates’ judgments of nonnative english-speaking teaching assistants;,citation_author=Donald L Rubin;,citation_publication_date=1992;,citation_cover_date=1992;,citation_year=1992;,citation_issue=4;,citation_volume=33;,citation_journal_title=Research in Higher Education;">
<meta name="citation_reference" content="citation_title=Social expectation improves speech perception in noise;,citation_author=Kevin B McGowan;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_issue=4;,citation_volume=58;,citation_journal_title=Language and Speech;,citation_publisher=Sage Publications Sage UK: London, England;">
<meta name="citation_reference" content="citation_title=Toward a cognitively realistic model of meaningful sociolinguistic variation;,citation_author=Kathryn Campbell-Kibler;,citation_editor=Anna Babel;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_inbook_title=Awareness and control in sociolinguistic research;">
<meta name="citation_reference" content="citation_title=Within-speaker variation in passing for a native speaker;,citation_author=Ksenia Gnevsheva;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_issue=2;,citation_volume=21;,citation_journal_title=International Journal of Bilingualism;,citation_publisher=SAGE Publications Sage UK: London, England;">
<meta name="citation_reference" content="citation_title=Experimental methods in sociolinguistics;,citation_author=Katie Drager;,citation_editor=Janet Holmes;,citation_editor=Kirk Hazen;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_inbook_title=Research methods in sociolinguistics: A practical guide;">
<meta name="citation_reference" content="citation_title=Implicitness and experimental methods in language variation research;,citation_author=Laura Rosseel;,citation_author=Stefan Grondelaers;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=s1;,citation_volume=5;,citation_journal_title=Linguistics Vanguard;,citation_publisher=De Gruyter Mouton;">
<meta name="citation_reference" content="citation_title=Reflections on the relation between direct/indirect methods and explicit/implicit attitudes;,citation_author=Nicolai Pharao;,citation_author=Tore Kristiansen;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=s1;,citation_volume=5;,citation_journal_title=Linguistics Vanguard;,citation_publisher=De Gruyter Mouton;">
<meta name="citation_reference" content="citation_title=How “deep” is dynamism? Revisiting the evaluation of moroccan-flavored netherlandic dutch;,citation_author=Stefan Grondelaers;,citation_author=Paul Gent;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=s1;,citation_volume=5;,citation_journal_title=Linguistics Vanguard;,citation_publisher=De Gruyter Mouton;">
<meta name="citation_reference" content="citation_title=Perceiving isn’t believing: Divergence in levels of sociolinguistic awareness;,citation_author=Kevin B McGowan;,citation_author=Anna M Babel;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=2;,citation_volume=49;,citation_journal_title=Language in Society;,citation_publisher=Cambridge University Press;">
<meta name="citation_reference" content="citation_title=A semiotic approach to awareness and control;,citation_author=Anna Babel;,citation_issue=1;,citation_volume=42;,citation_journal_title=Journal of Sociolinguistics;">
<meta name="citation_reference" content="citation_title=Perceiving gender while perceiving language: Integrating psycholinguistics and gender theory;,citation_author=Alayo Tripp;,citation_author=Benjamin Munson;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_issue=2;,citation_volume=13;,citation_journal_title=Wiley Interdisciplinary Reviews: Cognitive Science;,citation_publisher=Wiley Online Library;">
<meta name="citation_reference" content="citation_title=Recognizing uptalk: Memory and metalinguistic commentary for a sociolinguistic feature;,citation_author=Amelia Stecker;,citation_author=Annette D’Onofrio;,citation_issue=1;,citation_volume=42;,citation_journal_title=Journal of Sociolinguistics;">
<meta name="citation_reference" content="citation_title=Bidirectional effects of priming in speech perception: Social-to-lexical and lexical-to-social;,citation_author=Dominique A. Bouavichith;,citation_author=Ian C. Calloway;,citation_author=Justin T. Craft;,citation_author=Tamarae Hildebrandt;,citation_author=Stephen J. Tobin;,citation_author=Patrice S. Beddor;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_doi=10.1121/1.5101933;,citation_volume=145;,citation_journal_title=The Journal of the Acoustical Society of America;">
<meta name="citation_reference" content="citation_title=Deliberative control in audiovisual sociolinguistic perception;,citation_author=Kathryn Campbell-Kibler;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_issue=2;,citation_volume=25;,citation_journal_title=Journal of Sociolinguistics;,citation_publisher=Wiley Online Library;">
</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-intro" id="toc-sec-intro" class="nav-link active" data-scroll-target="#sec-intro">Introduction</a>
  <ul class="collapse">
  <li><a href="#coarticulatory-and-social-information-influence-ʃ-s-perception" id="toc-coarticulatory-and-social-information-influence-ʃ-s-perception" class="nav-link" data-scroll-target="#coarticulatory-and-social-information-influence-ʃ-s-perception">Coarticulatory and Social Information Influence [ʃ]-[s] perception</a></li>
  <li><a href="#phonetics-speech-perception-and-the-social-construction-of-gender" id="toc-phonetics-speech-perception-and-the-social-construction-of-gender" class="nav-link" data-scroll-target="#phonetics-speech-perception-and-the-social-construction-of-gender">Phonetics, Speech Perception, and the Social-Construction of Gender</a></li>
  <li><a href="#sub-mgt" id="toc-sub-mgt" class="nav-link" data-scroll-target="#sub-mgt">Matched Guise</a></li>
  </ul></li>
  <li><a href="#method" id="toc-method" class="nav-link" data-scroll-target="#method">Method</a>
  <ul class="collapse">
  <li><a href="#participants" id="toc-participants" class="nav-link" data-scroll-target="#participants">Participants</a></li>
  <li><a href="#stimulus-materials" id="toc-stimulus-materials" class="nav-link" data-scroll-target="#stimulus-materials">Stimulus Materials</a>
  <ul class="collapse">
  <li><a href="#auditory-stimuli" id="toc-auditory-stimuli" class="nav-link" data-scroll-target="#auditory-stimuli">Auditory Stimuli</a></li>
  </ul></li>
  <li><a href="#explicit-evaluations-of-auditory-stimuli" id="toc-explicit-evaluations-of-auditory-stimuli" class="nav-link" data-scroll-target="#explicit-evaluations-of-auditory-stimuli">Explicit Evaluations of Auditory Stimuli</a>
  <ul class="collapse">
  <li><a href="#visual-stimuli" id="toc-visual-stimuli" class="nav-link" data-scroll-target="#visual-stimuli">Visual Stimuli</a></li>
  </ul></li>
  <li><a href="#procedure" id="toc-procedure" class="nav-link" data-scroll-target="#procedure">Procedure</a></li>
  </ul></li>
  <li><a href="#predicted-results" id="toc-predicted-results" class="nav-link" data-scroll-target="#predicted-results">Predicted Results</a>
  <ul class="collapse">
  <li><a href="#face-male-or-female" id="toc-face-male-or-female" class="nav-link" data-scroll-target="#face-male-or-female">Face: male or female</a></li>
  <li><a href="#congruence-pairing-of-face-and-voice" id="toc-congruence-pairing-of-face-and-voice" class="nav-link" data-scroll-target="#congruence-pairing-of-face-and-voice">Congruence: pairing of face and voice</a></li>
  <li><a href="#guise-hidden-or-unhidden" id="toc-guise-hidden-or-unhidden" class="nav-link" data-scroll-target="#guise-hidden-or-unhidden">Guise: Hidden or Unhidden</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a>
  <ul class="collapse">
  <li><a href="#ʃ-s-percepts" id="toc-ʃ-s-percepts" class="nav-link" data-scroll-target="#ʃ-s-percepts">[ʃ]-[s] Percepts</a></li>
  <li><a href="#logistic-regression-and-quantitative-analysis" id="toc-logistic-regression-and-quantitative-analysis" class="nav-link" data-scroll-target="#logistic-regression-and-quantitative-analysis">Logistic Regression and Quantitative Analysis</a></li>
  <li><a href="#response-times" id="toc-response-times" class="nav-link" data-scroll-target="#response-times">Response Times</a></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a>
  <ul class="collapse">
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="index.docx"><i class="bi bi-file-word"></i>MS Word (apaquarto)</a></li></ul></div><div class="quarto-alternate-notebooks"><h2>Notebooks</h2><ul><li><a href="notebooks/explore-earthquakes-preview.html"><i class="bi bi-journal-code"></i>Explore Earthquakes</a></li></ul></div></nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">




<br>

<br>

<section id="title" class="level1 title unnumbered unlisted">
<h1 class="title unnumbered unlisted">Removing the disguise: the matched guise technique and listener awareness</h1>
<div class="Author">
<br>

<p>Kyler Laycock<sup>1</sup> and&nbsp;Kevin B McGowan<sup>2</sup></p>
<p><sup>1</sup>The Ohio State University</p>
<p><sup>2</sup>University of Kentucky</p>
</div>
</section>
<section id="author-note" class="level1 unnumbered unlisted AuthorNote">
<h1 class="unnumbered unlisted AuthorNote">Author Note</h1>
</section>
<section id="abstract" class="level1 unnumbered unlisted AuthorNote">
<h1 class="unnumbered unlisted AuthorNote">Abstract</h1>
<p><em>Keywords</em>:</p>
</section>
<section id="firstheader" class="level1 title unnumbered unlisted">
<h1 class="title unnumbered unlisted">Removing the disguise: the matched guise technique and listener awareness</h1>
</section>
<section id="sec-intro" class="level1">
<h1>Introduction</h1>
<p>There is abundant, converging evidence from experimental, ethnographic, and sociocultural approaches to the study of language that gender is performed by talkers and perceived by interlocutors through a stylistic bricolage <span class="citation" data-cites="zimman2017">(<a href="#ref-zimman2017" role="doc-biblioref">Zimman, 2017</a>)</span> comprising both non-linguistic and linguistic resources <span class="citation" data-cites="barrett2014 bucholtz2002">(<a href="#ref-barrett2014" role="doc-biblioref">Barrett, 2014</a>; <a href="#ref-bucholtz2002" role="doc-biblioref">Bucholtz, 2002</a>)</span>. Gender is a culturally-situated practice, and, crucially, social meaning is performed by embodied voices that simultaneously produce the distinctions necessary for linguistic meaning <span class="citation" data-cites="hall2021language podesvaKajino2014 bucholtzHall2016">(<a href="#ref-bucholtzHall2016" role="doc-biblioref">Bucholtz &amp; Hall, 2016</a>; <a href="#ref-hall2021language" role="doc-biblioref">Hall et al., 2021</a>; <a href="#ref-podesvaKajino2014" role="doc-biblioref">Podesva &amp; Kajino, 2014a</a>)</span>. This intersection of the construction of social and linguistic meaning via precise, dynamic speech articulation is perhaps nowhere more evident than in the palato-alveolar and alveolar fricative categories, [ʃ] and [s], in words like <em>ship</em> and <em>sip</em> in English <span class="citation" data-cites="strand1999 mackMunson2012b calder2018">(<a href="#ref-calder2018" role="doc-biblioref">Calder, 2018</a>; <a href="#ref-mackMunson2012b" role="doc-biblioref">Mack &amp; Munson, 2012</a>; <a href="#ref-strand1999" role="doc-biblioref">Strand, 1999</a>)</span>.</p>
<p>Articulatorily, these fricatives differ in the distance between the point of lingual articulation and the teeth. The size of the resulting space behind the teeth gives these sounds their characteristic sibilance <span class="citation" data-cites="fant1960 shadle1991">(<a href="#ref-fant1960" role="doc-biblioref">Fant, 1960</a>; <a href="#ref-shadle1991" role="doc-biblioref">Shadle, 1991</a>)</span>. English [s] has a short resonating chamber behind the teeth; it is typically produced by holding the tongue tip near enough to the alveolar ridge to cause relatively high frequency turbulent airflow. English [ʃ] has a comparatively larger resonating chamber; it is typically produced with a more posterior, palato-alveolar tongue position to cause turbulent airflow lower than [s] for the same talker. Concomittant with this articulatory difference for English listeners is a cultural association of masculinity with larger, longer vocal tracts and femininity with smaller, shorter vocal tracts <span class="citation" data-cites="may1976 ohala1994 eckert2012">(<a href="#ref-eckert2012" role="doc-biblioref">Eckert, 2012</a>; <a href="#ref-may1976" role="doc-biblioref">May, 1976</a>; <a href="#ref-ohala1994" role="doc-biblioref">Ohala, 1994</a>)</span>. [s] produced from a larger vocal tract will typically be lower in frequency than [s] produced from a smaller vocal tract, and listeners know this <span class="citation" data-cites="may1976">(<a href="#ref-may1976" role="doc-biblioref">May, 1976</a>)</span>.</p>
<p>A commonly used methodology in speech perception research involves the creation of synthetic fricative continua. These continua have endpoints in prototypical examples of [ʃ] and [s] with some number of equal-sized acoustic steps spliced, synthesized, or even mixed between these. Somewhere in the middle of such a continuum will be fricative-like noise that is ambiguous as to category membership: not clearly a [ʃ] and not clearly an [s]. <span class="citation" data-cites="may1976">May (<a href="#ref-may1976" role="doc-biblioref">1976</a>)</span> paired a continuum from [ʃ] (2.9 kHz) to [s] (4.4 kHz) with synthetic [æ] vowels to form simple CV syllables. May found that listeners perceived a higher proportion of the fricative continuum as [ʃ] when paired with vowel stimuli from a smaller vocal tract. The logic here is that smaller resonating chambers between the lingual articulation and teeth will have a higher mean frequency than larger resonating chambers. Listeners’ use of apparent vocal tract size in perception reflect their knowledge of this variation <span class="citation" data-cites="munson2011">(<a href="#ref-munson2011" role="doc-biblioref">Munson, 2011</a>)</span>.</p>
<p>Previous research in sociophonetic perception has established that listeners are so acutely sensitive to the alignment of these acoustic facts and cultural associations that perceived gender and fricative category participate in a relationship that is highly reminiscent of a phonetic trading relation <span class="citation" data-cites="repp1982">(<a href="#ref-repp1982" role="doc-biblioref">Repp, 1982</a>)</span> such that, for example, fricative sounds consistent with a larger vocal tract are perceived as more masculine <span class="citation" data-cites="bouavichithEtAl2019">(<a href="#ref-bouavichithEtAl2019" role="doc-biblioref">Bouavichith et al., 2019</a>)</span> and, in tandem, believing that a talker identifies as male can lead listeners to perceive more [ʃ]-like sounds as [s] <span class="citation" data-cites="strandJohnson1996 munson2011">(<a href="#ref-munson2011" role="doc-biblioref">Munson, 2011</a>; <a href="#ref-strandJohnson1996" role="doc-biblioref">Strand &amp; Johnson, 1996</a>)</span>.</p>
<p>The goal of the present study is to take advantage of this sociophonetic trading relation in listeners’ fricative categories to explore the role of awareness in socially-informed speech perception<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. It is well established that social information can influence how listeners perceive <span class="citation" data-cites="foulkesDocherty2006">(<a href="#ref-foulkesDocherty2006" role="doc-biblioref">Foulkes &amp; Docherty, 2006</a>)</span>, retrieve <span class="citation" data-cites="walkerHay2011">(<a href="#ref-walkerHay2011" role="doc-biblioref">Walker &amp; Hay, 2011</a>)</span>, and even remember <span class="citation" data-cites="nygaard1994">(<a href="#ref-nygaard1994" role="doc-biblioref">Nygaard et al., 1994</a>)</span> the linguistic aspect of the speech signal. However, because our knowledge of these phenomena come from disparate intellectual traditions, working with a range of quantitative and qualitative methods, with differing assumptions about the role of introspective awareness during the integration of social and linguistic information, one can come away from a detailed, rigorous review of the sociolinguistics and phonetics literature simultaneously convinced that listeners’ use of social information happens both obligatorily above and below the level of conscious awareness. XXX consider adding citations for each of these things: “disparate intellectual traditions, working with a range of quantitative and qualitative methods, with differing assumptions about the role of introspective awareness during the integration of social and linguistic information” , but generally yes this still feels true and reasonable XXX</p>
<section id="coarticulatory-and-social-information-influence-ʃ-s-perception" class="level2">
<h2 data-anchor-id="coarticulatory-and-social-information-influence-ʃ-s-perception">Coarticulatory and Social Information Influence [ʃ]-[s] perception</h2>
<p>Listeners are sensitive to these socially-informative patterns of [ʃ]-[s] variation, but it is important to understand how similar this sensitivity is to what has previously been observed in segmental speech perception. Just as vocal tract size can alter the frequencies of fricatives <span class="citation" data-cites="may1976">(e.g. <a href="#ref-may1976" role="doc-biblioref">May, 1976</a>)</span>, so too can coarticulation with a following vowel. Due to both place of articulation of the vowel and a change in lip rounding, the fricative in <em>see</em> [si] or <em>she</em> [ʃi] will sound higher than the fricative in <em>sue</em> [su] or <em>shoe</em> [ʃu] <span class="citation" data-cites="MannRepp1980 kunisakifujisaki1977 whalen1981">(<a href="#ref-kunisakifujisaki1977" role="doc-biblioref">Kunisaki &amp; Fujisaki, 1977</a>; <a href="#ref-MannRepp1980" role="doc-biblioref">Mann &amp; Repp, 1980</a>; <a href="#ref-whalen1981" role="doc-biblioref">Whalen, 1981</a>)</span>. <span class="citation" data-cites="whalen1984">Whalen (<a href="#ref-whalen1984" role="doc-biblioref">1984</a>)</span> paired synthesized vowels with incongruously coarticulated fricatives and found that, although researchers could not consciously identify the mismatched stimuli, participants nevertheless showed longer reaction times due to these coarticulatory mismatches. Listeners will readily fill-in missing or ambiguous information, the presence of actively <em>incongruous</em> articulatory information slows listener judgments.</p>
<p>Working in the context of segmental speech perception, <span class="citation" data-cites="MannRepp1980">Mann and Repp (<a href="#ref-MannRepp1980" role="doc-biblioref">1980</a>)</span> replicated May’s (1976) finding, extending it to natural productions of vowels spoken by a male or female-identified talker. Similar to May’s results with simulated vocal tract size, Mann &amp; Repp found a higher proportion of the fricative continuum was heard as [ʃ] when paired with the speech of the female talker. This early work, as was common in the period <span class="citation" data-cites="ohala1984">(<a href="#ref-ohala1984" role="doc-biblioref">Ohala, 1984</a>)</span>, theorized size as being a relatively deterministic feature of talker sexual dimorphism. One consequence of this view is that gender-related variation in the speech signal was considered mechanistic, universal, and following from purely physical laws. If vocal tract size is presumably not available for individual performance then listener knowledge of this variation can be correspondingly simple. Vocal tract size may influence perception, but it does so implicitly, automatically, and below the level of introspective awareness.</p>
<p><span class="citation" data-cites="strandJohnson1996">Strand and Johnson (<a href="#ref-strandJohnson1996" role="doc-biblioref">1996</a>)</span> conducted a pair of experiments investigating the influence of purported gender of a talker on the perception of the [ʃ]-[s] boundary. In their first experiment, listeners heard a [ʃ]-[s] continuum paired with voices previously normed as prototypically female, non-prototypically female, prototypically male, and non-prototypically male. The result replicates <span class="citation" data-cites="MannRepp1980">Mann and Repp (<a href="#ref-MannRepp1980" role="doc-biblioref">1980</a>)</span> and extends it to show that the influence of a gendered voice correlates with the protypicality of that voice. Their second experiment finds that presenting listeners with prototypically-gendered videos of their purported talker can, again, shift perceptions of the [ʃ]-[s] such that listeners report hearing a higher proportion of the continuum as [ʃ] when watching a female talker and a higher proportion of the same continuum as [s] when watching a male talker.</p>
<p>This AV condition is reminiscent of <span class="citation" data-cites="McGurkMacDonald1976">McGurk and MacDonald (<a href="#ref-McGurkMacDonald1976" role="doc-biblioref">1976</a>)</span> and is presented in that context. In the McGurk Effect, listeners presented with, for example, video of a person pronouncing the syllable [ga], paired with audio of the syllable [ba] will experience a third, fused, percept [da]. A striking feature of this effect is its automaticity; participants can not choose to perceive the two components of a fused percept independently. Awareness of the manipulation does not undermine the effect. Indeed, <span class="citation" data-cites="greenKuhlMeltzoffStevens1991">Green et al. (<a href="#ref-greenKuhlMeltzoffStevens1991" role="doc-biblioref">1991</a>)</span> found that the McGurk Effect succeeds even when listeners know that the visual talker and the auditory talker can not be the same person. McGurk can occur below the level of introspective awareness or, with instruction, above the level of introspective awareness. However, listeners, even with awareness, can not control their experience of the effect.</p>
<p>Listeners’ phonetic judgments, whether above or below the level of conscious awareness, depend on a rich constellation of evidence and expectation. Vocal tract size, following vowel quality, coarticulatory cues, and visual information, along with the acoustic properties of the coarticulated fricative itself, can all shape how listeners report experiencing a particular fricative. Rather than relying on a single, invariant, phonetic cue, listeners take the entire fricative and context into account <span class="citation" data-cites="whalen1991">(<a href="#ref-whalen1991" role="doc-biblioref">Whalen, 1991</a>)</span>. It is conceivable that such exquisite sensitivity to the phonetic cues conveying linguistic category membership might somehow restrict language users’ freedom to communicate and perceive social information via the same phonetic signal. This would be the prediction of a phonetic theory in which linguistic information and social information share the phonetic signal in a kind of zero sum game –where listeners must normalize away social variation to recover linguistic information or lose linguistic information in favor of the social. Instead, with these fricatives at least, we can observe the opposite. The fricatives [ʃ] and [s] often carry social meaning <span class="citation" data-cites="podesvakajino2014 mackMunson2012b">(<a href="#ref-mackMunson2012b" role="doc-biblioref">Mack &amp; Munson, 2012</a>; <a href="#ref-podesvakajino2014" role="doc-biblioref">Podesva &amp; Kajino, 2014b</a>)</span> with [s] being “perhaps the most iconic phonetic variable in the field” <span class="citation" data-cites="calder2018">(<a href="#ref-calder2018" role="doc-biblioref">Calder, 2018</a>)</span>. The implication is that the social and linguistic meanings of particular phonetic cues are not necessarily in competition with one another.</p>
<p>It is unclear from <span class="citation" data-cites="strandJohnson1996">Strand and Johnson (<a href="#ref-strandJohnson1996" role="doc-biblioref">1996</a>)</span> and subsequent work whether the perceptual influence of visually-presented social information about gender is implicit and automatic, as observed with coarticulation, vocal tract size, and the McGurk effect or whether the effect is altered (or diminished) when listeners are made aware of the manipulation and their attention is drawn to socially-meaningful variables <span class="citation" data-cites="labovEtAl2011">(<a href="#ref-labovEtAl2011" role="doc-biblioref">Labov et al., 2011</a>)</span>. The present work seeks to resolve this cognitive question to better understand precisely how the stylistic bricolage of gender is perceived and how gender perception functions in interaction. How do linguistic and non-linguistic resources interact during perception and, finally, what happens when these signals conflict? In order to conduct this study, however, it is necessary to be precise about how we conceive of and operationalize gender for the purposes of a speech perception experiment.</p>
</section>
<section id="phonetics-speech-perception-and-the-social-construction-of-gender" class="level2">
<h2 data-anchor-id="phonetics-speech-perception-and-the-social-construction-of-gender">Phonetics, Speech Perception, and the Social-Construction of Gender</h2>
<p>It has long seemed normal in phonetics to imagine that gender is a simple, binary projection from biological sex onto social identity <span class="citation" data-cites="daniel2007 samolinski2007">(<a href="#ref-daniel2007" role="doc-biblioref">Daniel et al., 2007</a>; <a href="#ref-samolinski2007" role="doc-biblioref">Samoliński et al., 2007</a>)</span>. However, if these biological tendencies were deterministic we would expect to see differentiation emerge only at puberty. It does not. In fact, prior to the onset of puberty, girls’ oral and nasal cavities tend to be larger than those of boys <span class="citation" data-cites="samolinski2007">(<a href="#ref-samolinski2007" role="doc-biblioref">Samoliński et al., 2007</a>)</span>. If anything, we should expect lower formants and lower center and peak frequencies for girls, inverting the adult pattern. Instead what we observe is that listeners can differentiate the voices of children as young as 4 years of age using vowel formant frequencies <span class="citation" data-cites="perryOhdeAshmead2001">(<a href="#ref-perryOhdeAshmead2001" role="doc-biblioref">Perry et al., 2001</a>)</span>. <span class="citation" data-cites="schellingerMunsonEdwards2017">Schellinger et al. (<a href="#ref-schellingerMunsonEdwards2017" role="doc-biblioref">2017</a>)</span> report a pair of experiments in which participants heard words produced by children between the ages of 2 and 5, and provided continuous ratings identifying fricatives, vowels, and gender typicality. Children typically show gendered patterns in speech at age 4 and up despite vocal tract length being non-distinct for this cohort. It is critical to remember that formants and fricatives are the result of not purely vocal tract biology but also articulator coordination. Even without biologically-differentiated vocal tracts, people who identify as male or female can perform that identity through gestural style. Vowels, in both their linguistic and social aspects, are the acoustic consequence of gestural control.</p>
<p>Gender is more likely the product of, rather than an explanation for, linguistic variation <span class="citation" data-cites="eckertPodesva2021">(<a href="#ref-eckertPodesva2021" role="doc-biblioref">Eckert &amp; Podesva, 2021</a>)</span>. Just as with words, genders are arbitrary; both the social labels and their acoustic correlates are language specific <span class="citation" data-cites="johnson2005 Johnson2006">(<a href="#ref-johnson2005" role="doc-biblioref">Johnson, 2005</a>; <a href="#ref-Johnson2006" role="doc-biblioref">Johnson, 2006</a>)</span> and the constellation of meanings are socially-constructed in interaction <span class="citation" data-cites="eckert2008">(<a href="#ref-eckert2008" role="doc-biblioref">Eckert, 2008</a>)</span>. The formant ratios that distinguish ‘male’ from ‘female’ in Norwegian are markedly different from the formant ratios that do this in Danish <span class="citation" data-cites="Johnson2006">(<a href="#ref-Johnson2006" role="doc-biblioref">Johnson, 2006</a>)</span>; what it means to be ‘male’ versus ‘female’ is quite different in Thailand than in Japan <span class="citation" data-cites="kang2013 alpert2014">(<a href="#ref-alpert2014" role="doc-biblioref">Alpert, 2014</a>; <a href="#ref-kang2013" role="doc-biblioref">Käng, 2013</a>)</span>. Children don’t perform adult-like vowel formant patterns because they were born tiny men and women, children perform adult-like vowel formant patterns because they identify as a gender and are using the cultural and linguistic resources available to communicate that gender to others. Humans are meaning-making agents, not deterministically resonating meat tubes.</p>
<p>In the earliest sociophonetic perception research it was still possible to imagine that the kind of knowledge listeners drew on to perceive gender was knowledge of primary biological traits. We now understand that, instead, the influence of gender-based expectations in speech perception is evidence of the influence of cultural knowledge on what might previously have been construed as purely linguistic decisions <span class="citation" data-cites="boydfruehwaldhall-lew_2021">(<a href="#ref-boydfruehwaldhall-lew_2021" role="doc-biblioref">Boyd et al., 2021</a>)</span>. Just as vowel quality, lip rounding, and syllable affiliation influence the perception of these fricatives, so too do socially-constructed gender categories.</p>
</section>
<section id="sub-mgt" class="level2">
<h2 data-anchor-id="sub-mgt">Matched Guise</h2>
<p>The Matched Guise technique (MGT) has been deployed in numerous configurations but, at its core, the technique pairs a single linguistic signal (identical recordings, an identical speaker, identical texts, etc.) with multiple purported social categories to elicit the influence of those cues on participants’ linguistic judgments <span class="citation" data-cites="campbell-kibler2005 campbell-kibler2007">(<a href="#ref-campbell-kibler2005" role="doc-biblioref">Campbell-Kibler, 2005</a>, <a href="#ref-campbell-kibler2007" role="doc-biblioref">2007</a>)</span> or language attitudes <span class="citation" data-cites="hadodoVolume chan2021">(<a href="#ref-chan2021" role="doc-biblioref">CHAN, 2021</a>; <a href="#ref-hadodoVolume" role="doc-biblioref">Hadodo, this volume</a>)</span>. In their foundational use of the technique, for example, <span class="citation" data-cites="lambertEtAl1960">Lambert et al. (<a href="#ref-lambertEtAl1960" role="doc-biblioref">1960</a>)</span> found that bilingual Montrealer’s voices evoked quite different social judgments in French vs English guises, providing evidence that listeners are able to perceive and connect social information in the voice to ideological framing of social types. In social speech perception research, cross-modal audio/visual matched guise studies are common in which visual information serves as a ‘guise’ for identical voice recordings; researchers sometimes disregard that even so-called standard voices carry social information <span class="citation" data-cites="rubin1992">Rubin (<a href="#ref-rubin1992" role="doc-biblioref">1992</a>)</span> and sometimes take the combination of voice and visual stimuli into account <span class="citation" data-cites="mcgowan2015 campbell-kibler2016 gnevsheva2017">(<a href="#ref-campbell-kibler2016" role="doc-biblioref">Campbell-Kibler, 2016</a>; <a href="#ref-gnevsheva2017" role="doc-biblioref">Gnevsheva, 2017</a>; <a href="#ref-mcgowan2015" role="doc-biblioref"><strong>mcgowan2015?</strong></a>)</span>. This latter type of guise manipulation has been called ‘inverted’ matched guise <span class="citation" data-cites="mcgowan2015">(<a href="#ref-mcgowan2015" role="doc-biblioref"><strong>mcgowan2015?</strong></a>)</span> or simply ‘identification’ <span class="citation" data-cites="drager2013">(<a href="#ref-drager2013" role="doc-biblioref">Drager, 2013</a>)</span>.</p>
<p>But uniting these linguistic researchers, and delineating them from colleagues in social psychology <span class="citation" data-cites="rosseelGrondelaers2019">(for discussion, see <a href="#ref-rosseelGrondelaers2019" role="doc-biblioref">Rosseel &amp; Grondelaers, 2019</a>)</span>, is the methodological assumption that the connection of voice to social type needs to happen below the level of conscious awareness. Awareness here, though generally not explicitly acknowledged, appears to be construed narrowly as participants’ ability to identify and comment on the existence of a guise manipulation. As researchers we demonstrate our assumption that the Matched Guise technique must be shielded from listener awareness through attempt to deceive participants about the intentional use of guise to elicit evidence of social evaluations, language attitudes, segmental speech perception, memory, etc.</p>
<p>Researchers go to great lengths to ensure this lack of awareness <span class="citation" data-cites="pharaoKristiansen2019 grondelaersVanGent2019">(<a href="#ref-grondelaersVanGent2019" role="doc-biblioref">Grondelaers &amp; Gent, 2019</a>; e.g. <a href="#ref-pharaoKristiansen2019" role="doc-biblioref">Pharao &amp; Kristiansen, 2019</a>)</span>. However, the majority of studies cannot speak to this lack of awareness during phonetic perception because the data provided by the participants is relatively late in processing and involves layers of potential introspection and evaluation that block access to the initial online percept for listeners and researchers alike. <span class="citation" data-cites="mcgowanBabel2020">McGowan and Babel (<a href="#ref-mcgowanBabel2020" role="doc-biblioref">2020</a>)</span> performed an audio/visual MGT with both a task designed to get at phonetic perception of individual segments and a sociolinguistic interview intended to investigate listeners’ judgements about the purported speaker. Every participant was shown both guises and while segmental and social perceptions were aligned with the identity of the purported talker in the initial guise presentation, these perceptions diverged in the second guise – with phonetic perceptions remaining unchanged and social evaluations tracking the change of guise. Of particular relevance to the present study, despite the fact that the fricatives used in <span class="citation" data-cites="mcgowanBabel2020">McGowan and Babel (<a href="#ref-mcgowanBabel2020" role="doc-biblioref">2020</a>)</span> were not different across guises, participants often commented on how the fricatives participated in communicating the purported social identity. This work raises the likelihood of at least two levels of sociophonetic perception and suggests that further work is needed to understand the role of awareness, and the necessity of deception, for the “complex, multi-layered process” of perception <span class="citation" data-cites="BabelVolume">(<a href="#ref-BabelVolume" role="doc-biblioref">Babel, this volume</a>)</span>.</p>
<p>This paper reports an audiovisual matched guise experiment with both standard ‘hidden’ and novel ‘unhidden’ instruction conditions. The basic task is a replication of <span class="citation" data-cites="strandJohnson1996">Strand and Johnson (<a href="#ref-strandJohnson1996" role="doc-biblioref">1996</a>)</span>. Listeners are asked to identify an ambiguous word as <em>sack</em> or <em>shack</em> on a [ʃ]-[s] continuum given manipulated beliefs about the gender identity of the talker <span class="citation" data-cites="trippMunson2022 steckerDOnofrioVolume">(<a href="#ref-steckerDOnofrioVolume" role="doc-biblioref">Stecker &amp; D’Onofrio, this volume</a>; <a href="#ref-trippMunson2022" role="doc-biblioref">Tripp &amp; Munson, 2022</a>)</span>. As described above, numerous previous replications have found that listeners perceive more of the ambiguous continuum as [ʃ] when they believe the speaker identifies as a woman and more as [s] when they believe the speaker identifies as a man and that, furthermore, this effect is bi-directional, with fricative type influencing perception of gender for an ambiguous voice <span class="citation" data-cites="bouavichithEtAl2019">(<a href="#ref-bouavichithEtAl2019" role="doc-biblioref">Bouavichith et al., 2019</a>)</span>. Unusually, participants in the present study’s ‘unhidden’ instruction condition were briefed, in the instructions, about the guise manipulation. They were instructed that the man or woman in the photo was not associated with the voice they were listening to. <span class="citation" data-cites="campbell-kibler2020">(<a href="#ref-campbell-kibler2020" role="doc-biblioref">Campbell-Kibler, 2021</a>)</span>, using a similar manipulation, finds that listeners have some ability to disregard social information when making accentedness or attractiveness judgments but that influence of available social information, particularly from the voice, is difficult to disregard completely. In the present study, participants were asked to provide a <em>sack</em>/<em>shack</em> lexical decision either with, or without, explicit instructions to disregard the visual stimulus.</p>
</section>
</section>
<section id="method" class="level1">
<h1>Method</h1>
<section id="participants" class="level2">
<h2 data-anchor-id="participants">Participants</h2>
<p>120 participants (self-identified 59 female, 61 male; ages 20 to 75) were recruited to complete the online experiment online. These participants were recruited through prolific.com and had provided language history and demographic data as part of Prolific’s general pre-screening questionnaire. Participation was restricted to a standard sample of desktop computer users located in the USA, , who spent most of their childhoods in the US, , with no known language or hearing difficulties. Additionally, due to an audio playback restriction imposed by Apple Computer, the Safari web browser could not be used. Participants were urged only to accept the task if they could do so in a quiet space, free from distractions and wearing headphones for the 6 to 10 minute duration of the experiment (average time 6:51). Headphone usage was not verified within the instrument. No participants’ data were excluded from analysis. Participants were paid $3 for their time, pro-rated from a projected rate of $20/hour (actual rate: $26.29/hour). This same instrument was piloted in the Speech Perception lab of The Ohio State University and, while reaction times online were generally slower than in-person, results from the online administration were generally consistent with results collected under laboratory conditions. Four participants were excluded for low accuracy rates (below 85%).</p>
</section>
<section id="stimulus-materials" class="level2">
<h2 data-anchor-id="stimulus-materials">Stimulus Materials</h2>
<section id="auditory-stimuli" class="level3">
<h3 data-anchor-id="auditory-stimuli">Auditory Stimuli</h3>
<p>The auditory stimuli used in this study are the same wav-format files used in . The stimuli, which were generously shared with us, contain two parts, both of which are drawn from synthetic continua: a fricative onset and a VC rime. The fricative onsets comprise a synthetic six step /-/ continuum. These steps were generated with the Klatt Synthesizer in Praat using parameters identical to ranging between the values of Munson’s second and eighth continuum steps (which were, in turn, based on the parameters used in ). Centers of Gravity ranged from 3.2 kHz (-like) to 7 kHz ().</p>
<p>For the VC rime, two additional continua were modified from natural productions of spoken by one male-identifying and one female-identifying talker in the carrier phrase “Say sack again”. These five-step continua were created by evenly spacing mean F0 across consecutive steps such that the male /k/ continuum increased F0 frequency and formant spacing from their unmodified values while the female talker’s /k/ continuum decreased both parameters from unmodified. each synthesized fricative token was concatenated with each CV rime of /k/ resulting in a total of 60 unique auditory stimuli. These manipulations are described in greater detail in Bouavichith et al’s section 2.1 and summarized visually in Figure <a href="#fig-stimuli" class="quarto-xref" aria-expanded="false">Figure&nbsp;1</a>. Unlike MGT studies that ask a talented, multi-dialectal talker to consciously change their speech style , these stimuli were asked to . As these talkers were advanced doctoral students in a linguistics PhD program, some of the elements of such an identity are likely available to conscious reflection, but many of these indexical features are likely implicit even for them.</p>
</section>
</section>
<section id="explicit-evaluations-of-auditory-stimuli" class="level2">
<h2 data-anchor-id="explicit-evaluations-of-auditory-stimuli">Explicit Evaluations of Auditory Stimuli</h2>
<p> voices carry social information, to better understand how our auditory stimuli might influence participants’ perception of the identities of the two talkers, . who participated in an in-person pilot version of the experiment were asked to make judgements regarding the gender, gender prototypicality, and sexuality of a natural production of produced by each of the two talkers. Participants listened to the recording and then selected from a fixed set of responses; no free form responses were elicited.</p>
<p>Participants’ judgments of the female voice indicate general agreement about the gender identity of the speaker. Most participants ( indicated the speaker’s gender to be female (2 participants further specified ‘trans-female’), and 3 were unsure or otherwise unable to determine the speaker’s gender. For the female voice, average prototypicality ratings (in which, for a given gender, 0 is least prototypical, and 5 is most prototypical) were 4.3/5 if the participant had indicated ‘female’, and 2.75/5 if the participant had indicated ‘trans female’. Judgements of the voice’s sexuality were more variable, with 54% indicating they were unsure, 40% indicating the speaker was most likely heterosexual, and 1 participant each indicating the speaker was most likely bisexual or another sexuality.</p>
<p>Participants’ judgments of the male voice suggest similar agreement. of participants indicated the speaker’s gender to be male, and 21% were unsure of the gender of the speaker. Average prototypicality ratings were lower for the male speaker but similarly consistent: 3.6/5 if the participant had indicated the voice belonged to a ‘male’ speaker, and 2/5 if they had indicated the person speaking was a ‘trans male’. As with the female voice, judgements of the voice’s sexuality were more variable. 65% indicated they were unsure, 14% indicated the speaker was most likely heterosexual, and 16% indicated homosexual and, again, 1 each indicating the speaker was most likely bisexual, or another sexuality not listed. </p>
<section id="visual-stimuli" class="level3">
<h3 data-anchor-id="visual-stimuli">Visual Stimuli</h3>
<p>The visual stimuli used in this study, again identical to the images used in <span class="citation" data-cites="bouavichithEtAl2019">(<a href="#ref-bouavichithEtAl2019" role="doc-biblioref">Bouavichith et al., 2019</a>)</span>, are shown in Figure <a href="#fig:stim" data-reference-type="ref" data-reference="fig:stim">[fig:stim]</a>. These included two face images, used for the guise manipulation, which were retrieved from the Chicago Face Database <span class="citation" data-cites="ChicagoFaceDatabase">(<a href="#ref-ChicagoFaceDatabase" role="doc-biblioref"><strong>ChicagoFaceDatabase?</strong></a>)</span>, a resource containing high-resolution, normed images of faces indexed by gender and ethnicity..</p>
<p>Additionally, two gray-scale line drawings were used as visual representations of <em>shack</em> and <em>sack</em>. These images were used in place of orthographic targets both to maintain consistency with Bouavichith et al’s design and to facilitate future eye tracking investigation of this phenomenon.</p>
<div class="center">
<p><img src="facesanddrawings.jpg" class="img-fluid" alt="image"> captionoffigureVisual Stimuli comprised <em>shack</em> and <em>sack</em> targets (top) and a gender-protypical ‘male’ and ‘female’ face (bottom)</p>
</div>
</section>
</section>
<section id="procedure" class="level2">
<h2 data-anchor-id="procedure">Procedure</h2>
<p>The experiment was created in OpenSesame v3.3 <span class="citation" data-cites="opensesame">(<a href="#ref-opensesame" role="doc-biblioref"><strong>opensesame?</strong></a>)</span> and exported for the web using OSWeb v1.4.14.0. Modifications to the experiment included translating portions of the python code into JavaScript and adding code to collect Prolific IDs and provide proof of completion to Prolific at the end of the experiment. This experiment was hosted on a JATOS <span class="citation" data-cites="JATOS">(<a href="#ref-JATOS" role="doc-biblioref"><strong>JATOS?</strong></a>)</span> instance hosted on an Ohio State University Linguistics Department server. Participants received a link to the experiment via Prolific and used their own computers, keyboards, and headphones to complete the experiment.</p>
<p>In a between-subjects design, participants were randomly assigned to one of two awareness conditions. These conditions differed only in the initial information provided as to the nature of the experiment. Participants in the <em>hidden</em> condition experienced a standard Matched Guise task. They were given no information about the task or the stimulus materials beyond the general instructions for completing the experiment: listen to the voice, press ‘z’ if you heard the word on the left, press ‘m’ for the word on the right. Participants in the <em>unhidden</em> condition also received this instruction and were given a partial debriefing regarding the task. They were informed that– while they would see faces onscreen while hearing words– the voices in a given trial were not produced by the person shown in the images, the images had been downloaded from a database of photographs created at the University of Chicago for experimental use, and that the auditory and visual stimuli were in no way related to each other. Participants were divided equally among these two conditions. Neither awareness condition was informed about the synthetic nature of the auditory stimuli.</p>
<p>Additionally, participants were assigned to one of two gender congruity conditions. Although the manipulated rimes sounded gender ambiguous to us, and had been rated as ambiguous by <span class="citation" data-cites="bouavichithEtAl2019">(<a href="#ref-bouavichithEtAl2019" role="doc-biblioref">Bouavichith et al., 2019</a>)</span>’s pilot participants, the possibility remained that the voices, particularly at the end-points, might be perceived incongruously with the faces as in, for example, <span class="citation" data-cites="McGowan2015">(<a href="#ref-McGowan2015" role="doc-biblioref"><strong>McGowan2015?</strong></a>)</span><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<p>In congruous trials, the faces and voices were paired such that participants were only presented with auditory stimuli from the female talker’s continuum alongside the female face and tokens from the male talker were only presented alongside the male face. In incongruous trials, by contrast, auditory stimuli from the female talker’s continuum were only ever presented alongside the male face and tokens from the male talker’s continuum were only ever presented alongside the female face. Half of participants were randomly assigned to each congruity condition, resulting in a 4-way between-subjects design across instruction and congruity conditions. Each participant heard all 60 auditory stimuli; 30 paired with the male face and 30 paired with the female face.</p>
<p>In each trial, participants were shown one of the two faces for 1500ms. Following this initial presentation, the face remained onscreen and was flanked by the <em>shack</em> and <em>sack</em> images. Simultaneously, one of the auditory stimuli was played over the headphones. The trial ended when the participant pressed an appropriate key on their physical keyboard and their response and reaction time data were uploaded to the JATOS instance. In both congruous and incongruous conditions, all 60 unique trials (30 per face) were presented twice to each participant for a total of 120 trials.</p>
</section>
</section>
<section id="predicted-results" class="level1">
<h1>Predicted Results</h1>
<section id="face-male-or-female" class="level2">
<h2 data-anchor-id="face-male-or-female">Face: male or female</h2>
<p>Consistent with previous results, we expect to replicate the Strand effect; in general, we anticipate that more of the [ʃ]-[s] continuum will be heard as [ʃ] when participants are shown the female face and more to be heard as [s] when participants are shown the male face. However, these general predictions about the Face presentation when the congruence of auditory and visual components of the guise are taken as a whole.</p>
</section>
<section id="congruence-pairing-of-face-and-voice" class="level2">
<h2 data-anchor-id="congruence-pairing-of-face-and-voice">Congruence: pairing of face and voice</h2>
<p>To our knowledge, the influence of congruence has not been directly investigated for listeners’ joint perception of gender and fricative place. <span class="citation" data-cites="johnsonstranddimperio1999">(<a href="#ref-johnsonstranddimperio1999" role="doc-biblioref"><strong>johnsonstranddimperio1999?</strong></a>)</span> tested AV integration of Male and Female faces with prototypical and non-prototypical gendered voices in a vowel quality perception task. They find what appears to be an incongruence effect with the prototypical male voice; listeners reported no difference in perceived vowel quality with this voice in either Face condition <span class="citation" data-cites="johnsonstranddimperio1999">(<a href="#ref-johnsonstranddimperio1999" role="doc-biblioref"><strong>johnsonstranddimperio1999?</strong></a>, Table 4)</span>. For this reason, we anticipate a replication of the Strand effect on fricative identification in our congruous trials (when Face and Voice do not conflict) but a failure to replicate for the incongruous trials (when Face and Voice provide conflicting social information). This difference may be stronger with the male voice, given both Johnson, Strand, and D’Imperio’s finding but also <span class="citation" data-cites="king2021">(<a href="#ref-king2021" role="doc-biblioref"><strong>king2021?</strong></a>)</span>.</p>
<p>We make a similar prediction for reaction times. <span class="citation" data-cites="johnsonstranddimperio1999">(<a href="#ref-johnsonstranddimperio1999" role="doc-biblioref"><strong>johnsonstranddimperio1999?</strong></a>)</span> did not collect reaction time data, but <span class="citation" data-cites="McGowan2011">(<a href="#ref-McGowan2011" role="doc-biblioref"><strong>McGowan2011?</strong></a>)</span> reports longer reaction times for incongruous trials, albeit in a very different task, and <span class="citation" data-cites="whalen1984">(<a href="#ref-whalen1984" role="doc-biblioref">Whalen, 1984</a>)</span> would seem to suggest that this should hold for listeners’ identification of fricatives on a [ʃ]-[s] continuum. Specifically, we predict longer reaction times, in general, for the Incongruous conditions. Furthermore, when gender information is most clear, at gender continuum steps 1 and 2 for the Male talker and at gender steps 4 &amp; 5 for the Female talker, and in conflict with the presented Face, listeners’ response times should be slower.</p>
<p>Since strong phonetic correlates of gender, F0 and F3, have been manipulated over the course of the VC rime continua in our auditory stimuli, we anticipate that the effect of incongruous face and voice should be strongest for the natural end points of the continua where the difference is most salient and weaker as phonetically-cued gender information becomes more ambiguous. These stimuli have been independently normed for ambiguity <span class="citation" data-cites="bouavichithEtAl2019">(<a href="#ref-bouavichithEtAl2019" role="doc-biblioref">Bouavichith et al., 2019, p. 1040</a>, Table 1)</span> in the 2nd and 3rd levels of the rime continua. This means we anticipate an interaction between Face and Rime step but only in the incongruous trials and only at the extremes of the rime continuum.</p>
</section>
<section id="guise-hidden-or-unhidden" class="level2">
<h2 data-anchor-id="guise-hidden-or-unhidden">Guise: Hidden or Unhidden</h2>
<p>The primary goal of this experiment was to explore the role of listener awareness and control in the matched guise technique. The tremendous care researchers take to ensure that the guise manipulation is hidden from participants suggests a kind of imagined fragility added[id=KL]of the effects of social information on language perception. From this view: listeners who become aware of the guise manipulation will have introspective access to and deliberative control over the influence of visual social information on perception. If this is true, explaining the guise manipulation, in the unhidden condition, should have a strongly negative effect on the Strand effect. Alternatively, if the influence of social information is not available to introspection or deliberative control, we should see no change between the (traditional) hidden matched guise and the unhidden guise.</p>
<p>Additionally, we speculate that there may be a response time difference between the Hidden and Unhidden guises even if there is no apparent difference in percept between the conditions. It can certainly be the case that participants will arrive at the same behavioral responses via different cognitive processing paths, perhaps drawing on different levels of knowledge and awareness, and that these differences may be visible in response times between the Instruction conditions.</p>
</section>
</section>
<section id="results" class="level1">
<h1>Results</h1>
<p>Participants provided a total of 14,400 trials (120 trials from each of 120 online participants ; 3600 trials in each instruction x congruity condition). It is not clear what it means to be ‘accurate’ when asked to perceive fricatives from a continuum so accuracy was calculated only for responses to the [ʃ] and [s] endpoints. Overall, participants were highly accurate (96.8%) but four participants were excluded from further analysis for accuracy below the pre-determined 85% threshold reducing the total number of trials to 13,920. Trials were coded as correct if the participant responded ‘shack’ to onset step 1 or ‘sack’ to onset step 6. The four excluded participants all scored 67.5% accuracy or lower.</p>
<p>An additional 50 trials were excluded due to response times that were either too fast or too slow. To reduce the effects of response time outliers on subsequent analyses, all response times shorter than 50 ms (N=0) and longer than 5000ms (N=50) were excluded. The 5000ms response time cutoff was used instead of imposing an in-experiment time limit on responses to a trial to ensure that participants were required to respond to each trial. Altogether, 530 trials were excluded, leaving data from 13,870 trials for analysis (approximately 96.3% of the initial data set). The majority (96.8%) of the remaining response times were within a range between 200 and 2000ms. To increase normality of the distribution of response times across participants, the remaining response times were log-transformed.</p>
<section id="ʃ-s-percepts" class="level2">
<h2 data-anchor-id="ʃ-s-percepts">[ʃ]-[s] Percepts</h2>
<div class="center">
<p><img src="Scurve.png" class="img-fluid" alt="image"> captionoffigureProportion ‘sack’ responses plotted as a function of [ʃ]-[s] fricative (Onset) continuum steps and purported gender presented by the face.</p>
</div>
<p>Figure <a href="#fig:scurve" data-reference-type="ref" data-reference="fig:scurve">[fig:scurve]</a> presents listeners’ percepts on this 2AFC task. The horizontal axis in each of these four plots is the fricative (syllable Onset) continuum step. Step 1 of the continuum is most [ʃ]-like, step 6 is the most [s]-like, steps 3 &amp; 4 are the most ambiguous. Darker lines in Figure <a href="#fig:scurve" data-reference-type="ref" data-reference="fig:scurve">[fig:scurve]</a> present trials using the female Face; lighter lines present trials using the male Face. The Hidden and Unhidden instruction conditions are represented by the left and right columns of figures, respectively. The rows present the Congruous blocks where Face and Coda speaker voice shared a gender identity (top) and Incongruous trials where Face and Coda speaker voice mismatched in gender identity (bottom).</p>
<p>A successful replication of the Strand effect would mean that a higher proportion of the ambiguous stimuli would be heard as [s] when the purported gender suggested by the face is male than when the face is female. This pattern appears to hold in both the Hidden and Unhidden conditions, but only when gender identity of the talker who produced the CV rime stimuli was congruous with the gender presented in the visual portion of the guise. From Figure <a href="#fig:scurve" data-reference-type="ref" data-reference="fig:scurve">[fig:scurve]</a> it would appear that listeners’ reported percepts more closely track the voice of the talker than the face in the picture when these sources of information are incongruous.</p>
<div class="center">
<p><img src="ambiguous-by-rime-step.png" class="img-fluid" alt="image"> captionoffigureProportion ‘sack’ responses on ambiguous fricative trials plotted as a function of CV rime continuum steps and gender identity of stimulus talker.</p>
</div>
<p>We predicted that, since strong phonetic correlates of gender have been manipulated over the course of the VC rime continua, the effect of incongruence should be strongest for the end points of the continua where the social information presented by the voice is, presumably, most salient and weaker as phonetically-cued gender information becomes more ambiguous. Figure <a href="#fig:rimes" data-reference-type="ref" data-reference="fig:rimes">[fig:rimes]</a> suggests that this prediction is at least partially borne out. Figure <a href="#fig:rimes" data-reference-type="ref" data-reference="fig:rimes">[fig:rimes]</a> plots proportion ‘sack’ responses to the ambiguous portion of the [ʃ]-[s] continuum (steps 3 &amp; 4) as a function of rime continuum step. The meaning of line color has changed in this figure. Dark lines represent the male talker and lighter lines represent the female talker. Step 1 on this continuum includes the most natural token for the male talker and the most manipulated token for the female talker while step 5 includes the most natural token for the male talker and the most manipulated token for the female talker. As before, columns present the Hidden and Unhidden conditions while rows present the Congruous and Incongruous blocks.</p>
<p>In a 2AFC task with unbiased stimuli, chance is 50%. Responses at the .5 line in figure <a href="#fig:rimes" data-reference-type="ref" data-reference="fig:rimes">[fig:rimes]</a> suggest that the ambiguous fricatives remained ambiguous while responses that tend to be above this line reflect a tendency toward [s] percepts and responses that tend to be below this line reflect a tendency toward [ʃ]. Across all 4 conditions we observe a declination from highest-proportion [s] responses in step 1 of the F0 continua to lowest in step 5. When face and voice were congruous, virtually all male-voice (and male face) responses are above or at 50% ‘sack’ and virtually all female-voiced (and female face) responses are at or <em>below</em> 50% ‘sack’. This is the same pattern that can be observed at Onset continuum steps 3 &amp; 4 in figure <a href="#fig:scurve" data-reference-type="ref" data-reference="fig:scurve">[fig:scurve]</a>. It is not clear from Figure <a href="#fig:rimes" data-reference-type="ref" data-reference="fig:rimes">[fig:rimes]</a> alone if there is any difference at all between the Congruous and Incongruous conditions. However, it is important to recall about the bottom row of this figure that male talker responses in the incongruous trials were presented with a female face while female talker trials were presented with a male face. Even a weakly-significant Strand effect would predict that the female talker, particularly on the more ambiguous continuum steps, should show more ‘sack’ responses consistent with having been shown a male face and no such effect is evident in this plot.</p>
<p>Indeed, a striking feature of figures <a href="#fig:scurve" data-reference-type="ref" data-reference="fig:scurve">[fig:scurve]</a> and <a href="#fig:rimes" data-reference-type="ref" data-reference="fig:rimes">[fig:rimes]</a> is how the apparent influence of gender information flips between congruous and incongruous conditions in the former but remains essentially constant in the latter. Taken together, these plots suggest that cues to gender in F0 is a stronger predictor of listeners’ reported percept in this matched guise task than just the purported gender of the face.</p>
<p>Finally, the main objective of this experiment was to explore the role of listener awareness in the matched guise technique. Here again there may be differences between the congruous and incongruous conditions that will be better understood through quantitative analysis, but the overall trend is clear. If there is an effect of explaining to participants that the voice and face in the matched guise task are unrelated to each other, that effect is so weak as to be essentially invisible in these visual interrogations of the data. Categorical responses in the Hidden and Unhidden instruction conditions appear to be identical.</p>
</section>
<section id="logistic-regression-and-quantitative-analysis" class="level2">
<h2 data-anchor-id="logistic-regression-and-quantitative-analysis">Logistic Regression and Quantitative Analysis</h2>
<p>These qualitative assessments of listener responses can be examined further through quantitative analysis. Through model comparison we initially arrived at a logistic mixed model to predict percept with Congruity condition, instruction condition, Onset step, Face, and Rime step with interactions for all but Rime step. This model was justified by model selection but given the notorious difficulty of interpreting a 4-way interaction and the preceding visual interrogation of the data, we opted to separate Congruence into a pair of 3-way models. Using <code>glmer()</code> <span class="citation" data-cites="lme4">(<a href="#ref-lme4" role="doc-biblioref"><strong>lme4?</strong></a>)</span>, we divided the data into congruous and incongruous subsets and fitted a pair of logistic mixed models (estimated using ML and BOBYQA optimizer) to predict percept with Instruction condition, Onset.step, Face and Rime step (<code>percept ~ Instruction * Onset.step * Face + Rime.step</code>). The models included random intercepts for subject. All categorical predictors were coded using contrast coding.</p>
<div class="center">
<p><img src="coefs_instruction.png" class="img-fluid" alt="image"> captionoffigureEstimated Beta coefficients for listener responses in the Congruous (black) and Incongruous (gray) logistic regression models plotted with 95% confidence intervals.</p>
</div>
<p>Beta coefficients for the two separate logistic mixed models are plotted together in Figure <a href="#fig:coefs" data-reference-type="ref" data-reference="fig:coefs">[fig:coefs]</a>. Terms plotted to the left of the dashed zero line have a negative influence on ‘sack’ percepts in the model while terms plotted to the right have a positive influence. As a consistency check we can observe that the levels of the Onset continuum behave in precisely the expected ways and all levels are statistically significant predictors of percept in both models. Onset step 1 ([ʃ]) is negatively associated with ‘sack’ responses and significant in both the Congruous (<span class="math inline">β=-5.00, SE=0.28, p &lt; 0.001</span>) and Incongruous (<span class="math inline">β=-4.84, SE=0.24, p &lt; 0.001</span>) models. Onset step 5 ([s]) is positively associated with ‘sack’ responses and significant in both the Congruous (<span class="math inline">β=4.35, SE=, p &lt; 0.001</span>) and Incongruous (<span class="math inline">β=4.12, SE=0.19, p &lt; 0.001</span>) models.</p>
<p>As visual inspection of the data suggests, this study includes a replication of the Strand effect in the Congruous condition. There is a main effect of Face in the model (<span class="math inline">β=-0.22, SE=0.09, p&lt;0.05</span>). Face is negatively associated with ‘sack’ responses suggesting that, with these stimuli, at least, it is more appropriate to understand the effect of Face as an increase of ‘shack’ responses given the female Face. The inclusion of the interaction term for Onset and Face allows us to see that the effect of Face is greatest on the ambiguous Onset steps 3 (<span class="math inline">β=-0.43, SE=0.11, p &lt; 0.001</span>) and, to a lesser extent, 4 (<span class="math inline">β=-0.23, SE=0.11, p &lt; 0.05</span>).</p>
<p>However, the Strand effect observed in the Congruous condition is not attributable entirely to the main effect of Face. Rime F0 is also significant; Rime level 1, the male end of the continuum, is positively associated with ‘sack’ responses (<span class="math inline">β=0.61, SE=0.10, p &lt; 0.001</span>) as is Rime level 2 (<span class="math inline">β=0.52, SE=0.10, p &lt; 0.001</span>). Rime level 3, where the continuum is most gender ambiguous, is not statistically significant. Rime level 4, on the female end of the continuum, is negatively associated with ‘sack’ responses and significant (<span class="math inline">β=-0.49, SE=0.10, p &lt; 0.001</span>).</p>
<p>Unsurprisingly, the Strand effect has not been replicated in the incongruous condition. As is visible in the bottom row of Figure <a href="#fig:scurve" data-reference-type="ref" data-reference="fig:scurve">[fig:scurve]</a>, the effect of Face on ‘sack’ responses is not significant. The interaction of Onset and Face also behaves quite differently in the Incongruous model. Onset x Face is negatively associated with ‘sack’ responses at Onset step 1 (<span class="math inline">β=-0.66, SE=0.24, p &lt; 0.001</span>) but positively associated with ‘sack’ responses and significant at Onset step 3 (<span class="math inline">β=0.27, SE=0.11, p &lt; 0.05</span>).</p>
<p>Interestingly, the significant effect of Rime observed in the Congruous model also holds, nearly identically, in the Incongruous model. Rime level 1, the male end of the continuum, is again positively associated with ‘sack’ responses (<span class="math inline">β=0.77, SE=0.10, p &lt; 0.001</span>) as is Rime level 2 (<span class="math inline">β=0.41, SE=0.10, p &lt; 0.001</span>). Rime level 3 is also not statistically significant in the Incongruous model. Rime level 4, on the female end of the continuum, is negatively associated with ‘sack’ responses and significant (<span class="math inline">β=-0.36, SE=0.10, p &lt; 0.001</span>).</p>
<p>Finally, the quantitative analysis of the primary objective of this experiment, exploring the effect of unhiding the matched guise manipulation from participants, largely supports the qualitative analysis. As can be observed in Figure <a href="#fig:coefs" data-reference-type="ref" data-reference="fig:coefs">[fig:coefs]</a>, there is no significant main effect of Instruction condition in either model. Still, a somewhat more nuanced picture emerges from the interactions of Instruction condition with Onset and the 3 way interaction of Instruction, Onset, and Face in the Congruous trials. The interaction of Instruction with Onset is significant, or nearly so, at every step of the fricative continuum other than the most significant. In the [ʃ]-like portion of the continuum, the interaction with face is positively associated with ‘sack’ responses at step 1 (<span class="math inline">β=0.65, SE=0.28, p &lt; 0.05</span>) and 2 (<span class="math inline">β=0.44, SE=0.18, p &lt; 0.05</span>). The interaction of guise with the most ambiguous onset step is not significant (<span class="math inline">β=0.011, SE=0.12</span>). The interaction of Instruction with Onset step 4, on the [s] end of the continuum is negatively associated with ‘sack’ responses and statistically significant (<span class="math inline">β=-0.43, SE=0.12, p &lt; 0.001</span>). Instruction x Onset step4 is also negatively associated with ‘sack’ responses but does not reach significance at the standard alpha level (<span class="math inline">β=-0.40, SE=0.22, p = 0.067</span>). The 3-way interaction of Instruction x Onset x Face is positively associated with ‘sack’ responses at step 2 (<span class="math inline">β=0.41, SE=0.17, p &lt; 0.05</span>) and weakly, but not significantly, negatively associated with ‘sack’ responses at step 5 (<span class="math inline">β=-0.38, SE=0.21, p = 0.080</span>).</p>
<p>There is also no main effect of Instruction in the Incongruous trials. The 3-way interaction of Instruction x Onset x Face, while justified by model selection for inclusion in this model, also does not reach statistical significance. However the 2-way interaction of Instruction with Onset step is positively associated with ‘sack’ responses at Onset step 2 (<span class="math inline">β=0.53, SE=0.21, p &lt; 0.05</span>) and approaches significance at step 3, where it is weakly positively associated (<span class="math inline">β=0.18, SE=0.11, p = 0.095</span>) and step 5 where it is weakly negatively associated (<span class="math inline">β=-0.32, SE=0.19, p = 0.086</span>).</p>
</section>
<section id="response-times" class="level2">
<h2 data-anchor-id="response-times">Response Times</h2>
<p>As with the logistic regression models, we again opted to separate Congruence into a pair of 3-way models for linear mixed model analysis of our log-transformed response time data. Using <code>lmer()</code> <span class="citation" data-cites="lme4">(<a href="#ref-lme4" role="doc-biblioref"><strong>lme4?</strong></a>)</span>, we reused the congruous and incongruous subsets created for the logistic regression models and We fitted a linear mixed model (estimated using REML and nloptwrap optimizer) to predict logRT with Guise, Onset, Face and Rime (<code>logRT ~ Instruction * Onset * Face + Rime</code>). The models included random intercepts for subject. All categorical predictors were coded using contrast coding. Beta coefficients for both models are plotted in Figure <a href="#fig:coefs:logRT" data-reference-type="ref" data-reference="fig:coefs:logRT">[fig:coefs:logRT]</a>. Terms plotted to the left of the zero line are associated with a decrease in log response time while terms plotted to the right of the zero line are associated with an increase in log response time. Notably, the longest response times are associated with the most ambiguous steps of the [ʃ]-[s] onset continuum. Onset step 3 is positively associated with response time and significant in both the congruous (<span class="math inline">β=0.08, SE=0.007, p &lt; 0.001</span>) and incongruous (<span class="math inline">β=0.07, SE=0.007, p &lt; 0.001</span> ) models. The same is true of step 4 in the congruous (<span class="math inline">β=0.07, SE=0.007, p &lt; 0.001</span>) and incongruous (<span class="math inline">β=0.07, SE=0.007, p &lt; 0.001</span>) models as well. On the other hand, steps 1, 2, and 5 are all negatively associated with response time and also significant in both models (see Figure <a href="#fig:coefs:logRT" data-reference-type="ref" data-reference="fig:coefs:logRT">[fig:coefs:logRT]</a>).</p>
<div class="center">
<p><img src="coefs-logRT_instructions.png" class="img-fluid" alt="image"> captionoffigureEstimated Beta coefficients for log-transformed response times in the Congruous (black) and Incongruous (gray) linear regression models plotted with 95% confidence intervals.</p>
</div>
<p>We predicted overall slower response times in the Incongruous than Congruous conditions and this prediction is not borne out by the data. Apart from generally higher variability in the incongruous conditions, there is no positive or negative trend in response times between the two Congruity models. For example, within the Incongruous model response times given the interaction of Onset step 3 * Face are longer (<span class="math inline">β=-0.009, SE=0.007, p = 0.17</span>), which would seem to support our prediction, but response times for Onset step 4 * Face are shorter (<span class="math inline">β-0.02, SE=0.007, p &lt; 0.01</span>), the opposite of what we predicted. The exact opposite pattern appears within the Congruous model where response times are shorter given Onset 3 * Face (<span class="math inline">β=-0.02, SE=0.007, p &lt; 0.01</span>) but longer given Onset step 4 * Face (<span class="math inline">β=0.04, SE=0.007, p &lt; 0.001</span>). These crossing patterns can be seen in Figure <a href="#fig:coefs:logRT" data-reference-type="ref" data-reference="fig:coefs:logRT">[fig:coefs:logRT]</a>.</p>
<p>Given the replication of the Strand effect in the Congruous, but not the Incongruous conditions described in the previous section, it may be notable that there is a significant main effect of Face in the Congruous model where it is negatively associated with response time (<span class="math inline">β=0.22, SE=0.08, p &lt; 0.05</span>) and not significant in the Incongruous model.</p>
</section>
</section>
<section id="discussion" class="level1">
<h1>Discussion</h1>
<p>The question that motivated this study was a desire to understand the role of listener awareness and control in the matched guise technique. We believe that the careful measures researchers generally employ to obscure the nature of the guise manipulation from participants is attributable to a long-held assumption in the sociolinguistics literature that social knowledge is high-level knowledge, available to introspective control, and that this differs from linguistic knowledge which is low-level knowledge, unavailable to control <span class="citation" data-cites="campbell-kibler2016">(<a href="#ref-campbell-kibler2016" role="doc-biblioref">Campbell-Kibler, 2016</a>)</span>. The results of the present study are inconsistent with this imagined fragility of the influence of social knowledge. Revealing the nature of the guise manipulation did not significantly influence listener responses in either the congruous or incongruous conditions. Nor did this revelation have a significant influence on response times in either condition.</p>
<p>The finding that the Matched Guise effect holds for speech perception both when hidden from the participant and when unhidden is inconsistent with a model of processing in which social knowledge simply acts as a filter on linguistic knowledge. Social knowledge influences perception even when listeners are aware that it is, or may be, false. This result parallels previous results for accentedness and attractiveness judgments <span class="citation" data-cites="campbell-kibler2020">(<a href="#ref-campbell-kibler2020" role="doc-biblioref">Campbell-Kibler, 2021</a>)</span>. A similar result may be present, for social information, in the within-participants guise manipulation of <span class="citation" data-cites="mcgowanBabel2020">(<a href="#ref-mcgowanBabel2020" role="doc-biblioref">McGowan &amp; Babel, 2020</a>)</span>. In that study, the authors use participants’ metalinguistic commentaries to assess the extent to which the guise manipulations were or were not ‘believed’. The results of the present study suggests that that belief may be irrelevant. The present result also gives additional context to studies demonstrating influence of social knowledge even when listeners have no reason to believe the guise manipulation <span class="citation" data-cites="Niedzielski1999 haynolandrager2006 HayDrager2010">(<a href="#ref-Niedzielski1999" role="doc-biblioref"><strong>Niedzielski1999?</strong></a>; <a href="#ref-haynolandrager2006" role="doc-biblioref"><strong>haynolandrager2006?</strong></a>; <a href="#ref-HayDrager2010" role="doc-biblioref"><strong>HayDrager2010?</strong></a>)</span>. It is unclear whether social knowledge will prove to be as resilient to awareness as the obligatory McGurk effect <span class="citation" data-cites="McGurkMacDonald1976">(<a href="#ref-McGurkMacDonald1976" role="doc-biblioref">McGurk &amp; MacDonald, 1976</a>)</span> which persists even when participants actively identify that the face and voice in the experiment are mismatched <span class="citation" data-cites="GreenEtAl1991">(<a href="#ref-GreenEtAl1991" role="doc-biblioref"><strong>GreenEtAl1991?</strong></a>)</span>, but the suggestion is that it will.</p>
<p>The gender identity of the talker who produced the VC Rime supplemented Face in the Congruous conditions to make the Strand effect even stronger; the mechanism may prove similar to the way lip-rounding accentuates the backness of back vowels. In the Incongruous conditions, though, listeners’ perception of the [ʃ]-[s] continuum tracked the VC Rimes, rather than the purported gender of the Face. This pattern was strongest in the least-ambiguous portions of the Rime continuum and weakest in the most-ambiguous. In a sense, by separating trials by congruity of face and voice we have replicated <span class="citation" data-cites="strandJohnson1996">(<a href="#ref-strandJohnson1996" role="doc-biblioref">Strand &amp; Johnson, 1996</a>)</span>’s exp1 and exp2 simultaneously. One wonders, looking back at their exp2, whether this classic result was <em>also</em> a congruous condition in which listeners had sufficient gender information from the voice to supplement the purported information from the Face. Even the non-prototypical voices used in that study did pattern, in exp1, in weakly gendered ways. This finding may provide some insight into recent failures to replicate the original Strand effect <span class="citation" data-cites="schellingerMunsonEdwards2017 wilbanks2022">(<a href="#ref-schellingerMunsonEdwards2017" role="doc-biblioref">Schellinger et al., 2017</a>; <a href="#ref-wilbanks2022" role="doc-biblioref"><strong>wilbanks2022?</strong></a>)</span>.</p>
<p>The phonetic correlates of gender manipulated in the VC rimes for this study are F0 and formant ratios. However, these may not be the only cues listeners are drawing upon with their knowledge of US English. Surely, F0 and vowel formant ratios <em>can be</em> important to listeners, just as voice onset time and vocal fold vibration can be important cues to the voicing of /t/ and /d/. But as <span class="citation" data-cites="lisker1986">(<a href="#ref-lisker1986" role="doc-biblioref"><strong>lisker1986?</strong></a>)</span> catalogs, there are 16 cues to this apparently simple feature in English, any of which might be sufficient to communicate voicing, but none of which is required. In this study we have used manipulated stimuli that obscure, over the course of two gender continua, the gender identity of the talker who produced the basis token for that continuum. At an explicit level, these continua <em>sound ambiguous</em> to the experimenters in much the way that <span class="citation" data-cites="whalen1984">(<a href="#ref-whalen1984" role="doc-biblioref">Whalen, 1984</a>)</span>’s stimuli do not sound obviously mismatched. But our perception results suggest that listeners are still aware, albeit implicitly, of the gender identity we have attempted to obscure by altering the phonetic correlates of gender.</p>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>Decades of research since <span class="citation" data-cites="strandJohnson1996">(<a href="#ref-strandJohnson1996" role="doc-biblioref">Strand &amp; Johnson, 1996</a>)</span>‘s original finding have demonstrated that a visual cue can shift fricative perceptions when paired with an ambiguously-gendered voice (although cf Munson 2017 and Wilbanks 2022). <span class="citation" data-cites="bouavichithEtAl2019">(<a href="#ref-bouavichithEtAl2019" role="doc-biblioref">Bouavichith et al., 2019</a>)</span> demonstrated with eye-tracking that this effect is fast and bi-directional. One could come away from Strand &amp; Johnson’s exp1 and exp2 and subsequent replications with a theoretical model in which visually-cued social information and phonetically-cued social information exert equivalent influence on speech perception. Prototypically-gendered voices can shift perception of a [ʃ]-[s] continuum and prototypically-gendered visual information can as well. However, listeners’ behavior in our Congruous and Incongruous conditions is inconsistent with such a model and suggests, instead, that when visually-cued and phonetically-cued social information are in congruence, they can enhance one another. When, on the other hand, these information sources conflict, it is the phonetically-cued social information that will dominate <span class="citation" data-cites="campbell-kibler2020">(<a href="#ref-campbell-kibler2020" role="doc-biblioref">Campbell-Kibler, 2021</a>)</span>.</p>
<p>It is unlikely that fricatives are unique in this respect. For example, the incongruous results seen in this study are, perhaps, predicted by lack of Face effect for <span class="citation" data-cites="johnsonstranddimperio1999">(<a href="#ref-johnsonstranddimperio1999" role="doc-biblioref"><strong>johnsonstranddimperio1999?</strong></a>)</span>’s vowel perception results in exp2 given a stereotypical face (particularly, in that study, for the male voice). As listeners, we do not have veridical access to the speech sounds intended by a talker. Instead, we must combine the speech signal with our phonological knowledge, lexical knowledge, social expectations, visual input, expectations of the social world <span class="citation" data-cites="BabelVolume">(<a href="#ref-BabelVolume" role="doc-biblioref">Babel, this volume</a>)</span> and other sensory information to arrive at a percept. The implication is that perception is more holistic than is dreamt of in our phonologies. Category boundaries, whether for speech sounds or social categories, are fuzzy and perception needs to be fast. We retain knowledge of, and use, detailed social and linguistic knowledge at both high and low levels of processing. Enumerating the phonetic correlates of gender may not be the wrong question, but it is certainly premature given the limitations of current theory to account for what listeners actually do. A better question is something like “what kinds of knowledge do listeners draw on during perception and when?”</p>
<p><span class="citation" data-cites="barrett2014">(<a href="#ref-barrett2014" role="doc-biblioref">Barrett, 2014, p. 205</a>)</span> writes, “any assumption of essentialism will ultimately marginalize those individuals who do not fit the essentialist understandings of human behavior”. It may not feel brutal or reductive to read <span class="citation" data-cites="May1976">(<a href="#ref-May1976" role="doc-biblioref"><strong>May1976?</strong></a>)</span>’s findings about large and small vocal tracts as if they refer to male and female vocal tracts, respectively, but it does necessarily imply that tall, long-necked women and short, squat-necked men need to find some other way of labeling themselves. The idea that male voices come from large bodies and female voices come from small bodies need not be literally true for the phonetic and perceptual correlates of size to become enregistered alongside other features in the creation of gendered personae (D’Onofrio 2020). Our prediction that incongruity in face and voice would slow listener judgments was not supported. It is tempting to interpret this as evidence that, unlike misleading coarticulatory information, listeners are aware of the diversity of gender expression, but this is not a question the current study can resolve.</p>
<p>What the current study can resolve is that listeners’ social knowledge of speech is not delicate. The present result is equally inconsistent with a model that disregards social knowledge entirely and with any model of speech perception that presumes <em>all</em> social knowledge to be late, high-level, and available to introspective control. Part of what listeners know when they know a language includes the simultaneous patterning of ‘linguistic’ and ‘social’ information in a shared phonetic signal. Social knowledge is not a weakly-associated prime; Social knowledge and linguistic knowledge are deeply intertwined in speech perception and it is perverse to assume that the language subsystem underlying this ability would necessarily distinguish them.</p>
<section id="references" class="level2 unnumbered">
<h2 class="unnumbered" data-anchor-id="references">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-alpert2014" class="csl-entry" role="listitem">
Alpert, E. R. (2014). <em>Language, gender, and ideology in japanese professional matchmaking.</em> [PhD thesis]. University of Michigan, Department of Anthropology.
</div>
<div id="ref-BabelVolume" class="csl-entry" role="listitem">
Babel, A. (this volume). A semiotic approach to awareness and control. <em>Journal of Sociolinguistics</em>, <em>42</em>(1).
</div>
<div id="ref-barrett2014" class="csl-entry" role="listitem">
Barrett, R. (2014). The emergence of the unmarked. In L. Zimman, J. Davis, &amp; J. Raclaw (Eds.), <em>Queer excursions: Retheorizing binaries in language, gender, and sexuality</em> (pp. 195–223). Oxford University Press.
</div>
<div id="ref-bouavichithEtAl2019" class="csl-entry" role="listitem">
Bouavichith, D. A., Calloway, I. C., Craft, J. T., Hildebrandt, T., Tobin, S. J., &amp; Beddor, P. S. (2019). Bidirectional effects of priming in speech perception: Social-to-lexical and lexical-to-social. <em>The Journal of the Acoustical Society of America</em>, <em>145</em>. <a href="https://doi.org/10.1121/1.5101933">https://doi.org/10.1121/1.5101933</a>
</div>
<div id="ref-boydfruehwaldhall-lew_2021" class="csl-entry" role="listitem">
Boyd, Z., Fruehwald, J., &amp; Hall-Lew, L. (2021). Crosslinguistic perceptions of /s/ among english, french, and german listeners. <em>Language Variation and Change</em>, <em>33</em>(2), 165–191. <a href="https://doi.org/10.1017/S0954394521000089">https://doi.org/10.1017/S0954394521000089</a>
</div>
<div id="ref-bucholtz2002" class="csl-entry" role="listitem">
Bucholtz, M. (2002). From <span>“sex differences”</span> to gender variation in sociolinguistics. <em>University of Pennsylvania Working Papers in Linguistics</em>, <em>8</em>(3), 33–45.
</div>
<div id="ref-bucholtzHall2016" class="csl-entry" role="listitem">
Bucholtz, M., &amp; Hall, K. (2016). Embodied sociolinguistics. <em>Sociolinguistics: Theoretical Debates</em>, <em>1</em>(1), 173–200.
</div>
<div id="ref-calder2018" class="csl-entry" role="listitem">
Calder, J. (2018). From <span>“gay lisp”</span> to <span>“fierce queen”</span>: The sociophonetics of sexuality’s most iconic variable. In K. Hall &amp; R. Barrett (Eds.), <em>The oxford handbook of language and sexuality</em> (pp. 1–23).
</div>
<div id="ref-campbell-kibler2005" class="csl-entry" role="listitem">
Campbell-Kibler, K. (2005). <em>Listener perceptions of sociolinguistic variables: The case of (ING)</em> [PhD thesis]. Stanford University.
</div>
<div id="ref-campbell-kibler2007" class="csl-entry" role="listitem">
Campbell-Kibler, K. (2007). Accent,(ING), and the social logic of listener perceptions. <em>American Speech</em>, <em>82</em>(1), 32–64.
</div>
<div id="ref-campbell-kibler2016" class="csl-entry" role="listitem">
Campbell-Kibler, K. (2016). Toward a cognitively realistic model of meaningful sociolinguistic variation. In A. Babel (Ed.), <em>Awareness and control in sociolinguistic research</em> (pp. 123–151).
</div>
<div id="ref-campbell-kibler2020" class="csl-entry" role="listitem">
Campbell-Kibler, K. (2021). Deliberative control in audiovisual sociolinguistic perception. <em>Journal of Sociolinguistics</em>, <em>25</em>(2), 253–271.
</div>
<div id="ref-chan2021" class="csl-entry" role="listitem">
CHAN, K. L. R. (2021). Verbal guise test: Problems and solutions. <em>Academia Letters</em>.
</div>
<div id="ref-daniel2007" class="csl-entry" role="listitem">
Daniel, M. M., Lorenzi, M. C., Costa Leite, C. da, &amp; Lorenzi-Filho, G. (2007). Pharyngeal dimensions in healthy men and women. <em>Clinics</em>, <em>62</em>(1), 5–10.
</div>
<div id="ref-drager2013" class="csl-entry" role="listitem">
Drager, K. (2013). Experimental methods in sociolinguistics. In J. Holmes &amp; K. Hazen (Eds.), <em>Research methods in sociolinguistics: A practical guide</em> (pp. 58–73). Wiley Blackwell.
</div>
<div id="ref-eckert2008" class="csl-entry" role="listitem">
Eckert, P. (2008). Variation and the indexical field 1. <em>Journal of Sociolinguistics</em>, <em>12</em>(4), 453–476.
</div>
<div id="ref-eckert2012" class="csl-entry" role="listitem">
Eckert, P. (2012). Three waves of variation study: <span>The</span> emergence of meaning in the study of sociolinguistic variation. <em>Annual Review of Anthropology</em>, <em>41</em>(1), 87–100.
</div>
<div id="ref-eckertPodesva2021" class="csl-entry" role="listitem">
Eckert, P., &amp; Podesva, R. J. (2021). Non-binary approaches to gender and sexuality. <em>The Routledge Handbook of Language, Gender, and Sexuality</em>, 25–36.
</div>
<div id="ref-fant1960" class="csl-entry" role="listitem">
Fant, G. (1960). <em>Acoustic theory of speech production</em>. Mouton.
</div>
<div id="ref-foulkesDocherty2006" class="csl-entry" role="listitem">
Foulkes, P., &amp; Docherty, G. (2006). The social life of phonetics and phonology. <em>Journal of Phonetics</em>, <em>34</em>, 409–438.
</div>
<div id="ref-gnevsheva2017" class="csl-entry" role="listitem">
Gnevsheva, K. (2017). Within-speaker variation in passing for a native speaker. <em>International Journal of Bilingualism</em>, <em>21</em>(2), 213–227.
</div>
<div id="ref-greenKuhlMeltzoffStevens1991" class="csl-entry" role="listitem">
Green, K., Kuhl, P., Meltzoff, A., &amp; Stevens, E. (1991). Integrating speech information across talkers, gender, and sensory modality: <span>Female</span> faces and male voices in the <span>McGurk</span> effect. <em>Attention, Perception, &amp; Psychophysics</em>, <em>50</em>(6), 524–536. <a href="http://dx.doi.org/10.3758/BF03207536">http://dx.doi.org/10.3758/BF03207536</a>
</div>
<div id="ref-grondelaersVanGent2019" class="csl-entry" role="listitem">
Grondelaers, S., &amp; Gent, P. van. (2019). How <span>“deep”</span> is dynamism? Revisiting the evaluation of moroccan-flavored netherlandic dutch. <em>Linguistics Vanguard</em>, <em>5</em>(s1).
</div>
<div id="ref-hadodoVolume" class="csl-entry" role="listitem">
Hadodo, M. (this volume). Situating experience in social meaning: Ethnography, experiments and exemplars in the enregisterment of istanbul greek. <em>Journal of Sociolinguistics</em>, <em>42</em>(1).
</div>
<div id="ref-hall2021language" class="csl-entry" role="listitem">
Hall, K., Borba, R., &amp; Hiramoto, M. (2021). Language and gender. <em>The International Encyclopedia of Linguistic Anthropology</em>, 892–912.
</div>
<div id="ref-johnson2005" class="csl-entry" role="listitem">
Johnson, K. (2005). Speaker normalization in speech perception. In D. B. Pisoni &amp; R. Remez (Eds.), <em>The handbook of speech perception</em> (pp. 363–389).
</div>
<div id="ref-Johnson2006" class="csl-entry" role="listitem">
Johnson, K. (2006). Resonance in an exemplar-based lexicon: The emergence of social identity and phonology. <em>Journal of Phonetics</em>, <em>34</em>, 485–499.
</div>
<div id="ref-kang2013" class="csl-entry" role="listitem">
Käng, D. B. (2013). Conceptualizing thai genderscapes: Transformation and continuity in the thai sex/gender system. In <em>Contemporary socio-cultural and political perspectives in thailand</em> (pp. 409–429). Springer.
</div>
<div id="ref-kunisakifujisaki1977" class="csl-entry" role="listitem">
Kunisaki, O., &amp; Fujisaki, H. (1977). On the influence of context upon perception of voiceless fricative consonants. <em>Annual Bulletin</em>, <em>11</em>, 85–91.
</div>
<div id="ref-labovEtAl2011" class="csl-entry" role="listitem">
Labov, W., Ash, S., Ravindranath, M., Weldon, T., Baranowski, M., &amp; Nagy, N. (2011). Properties of the sociolinguistic monitor. <em>Journal of Sociolinguistics</em>, <em>15</em>(4), 431–463.
</div>
<div id="ref-lambertEtAl1960" class="csl-entry" role="listitem">
Lambert, W. E., Hodgson, R. C., Gardner, R. C., &amp; Fillenbaum, S. (1960). Evaluational reactions to spoken languages. <em>The Journal of Abnormal and Social Psychology</em>, <em>60</em>(1), 44.
</div>
<div id="ref-mackMunson2012b" class="csl-entry" role="listitem">
Mack, S., &amp; Munson, B. (2012). The association between/s/quality and perceived sexual orientation of men’s voices: Implicit and explicit measures. <em>Journal of Phonetics</em>, <em>40</em>(1), 198–212.
</div>
<div id="ref-MannRepp1980" class="csl-entry" role="listitem">
Mann, V. A., &amp; Repp, B. H. (1980). Influence of vocalic context on perception of the [ʃ]-[s] distinction. <em>Perception &amp; Psychophysics</em>, <em>28</em>(3), 213–228.
</div>
<div id="ref-may1976" class="csl-entry" role="listitem">
May, J. (1976). Vocal tract normalization for /s/ and /š/. <em>Haskins Laboratories Status Report on Speech Research</em>, <em>SR-48</em>, 67–73.
</div>
<div id="ref-mcgowanBabel2020" class="csl-entry" role="listitem">
McGowan, K. B., &amp; Babel, A. M. (2020). Perceiving isn’t believing: Divergence in levels of sociolinguistic awareness. <em>Language in Society</em>, <em>49</em>(2), 231–256.
</div>
<div id="ref-McGurkMacDonald1976" class="csl-entry" role="listitem">
McGurk, H., &amp; MacDonald, J. (1976). Hearing lips and seeing voices. <em>Nature</em>, <em>264</em>, 746–748.
</div>
<div id="ref-munson2011" class="csl-entry" role="listitem">
Munson, B. (2011). The influence of actual and imputed talker gender on fricative perception, revisited (l). <em>The Journal of the Acoustical Society of America</em>, <em>130</em>(5), 2631–2634.
</div>
<div id="ref-nygaard1994" class="csl-entry" role="listitem">
Nygaard, L. C., Sommers, M. S., &amp; Pisoni, D. B. (1994). Speech perception as a talker-contingent process. <em>Psychological Science</em>, <em>5</em>(1), 42–46.
</div>
<div id="ref-ohala1984" class="csl-entry" role="listitem">
Ohala, J. J. (1984). An ethological perspective on common cross-language utilization of F₀ of voice. <em>Phonetica</em>, <em>41</em>(1), 1–16.
</div>
<div id="ref-ohala1994" class="csl-entry" role="listitem">
Ohala, J. J. (1994). The frequency code underlies the sound-symbolic use of voice pitch. <em>Sound Symbolism</em>, 325–347.
</div>
<div id="ref-perryOhdeAshmead2001" class="csl-entry" role="listitem">
Perry, T. L., Ohde, R. N., &amp; Ashmead, D. H. (2001). The acoustic bases for gender identification from children’s voices. <em>The Journal of the Acoustical Society of America</em>, <em>109</em>(6), 2988–2998.
</div>
<div id="ref-pharaoKristiansen2019" class="csl-entry" role="listitem">
Pharao, N., &amp; Kristiansen, T. (2019). Reflections on the relation between direct/indirect methods and explicit/implicit attitudes. <em>Linguistics Vanguard</em>, <em>5</em>(s1).
</div>
<div id="ref-podesvaKajino2014" class="csl-entry" role="listitem">
Podesva, R. J., &amp; Kajino, S. (2014a). Sociophonetics, <span>Gender</span>, and <span>Sexuality</span>. In S. Ehrlich, M. Meyerhoff, &amp; J. Holmes (Eds.), <em>The <span>Handbook</span> of <span>Language</span>, <span>Gender</span>, and <span>Sexuality</span></em> (pp. 103–122). John Wiley &amp; Sons, Inc. <a href="https://doi.org/10.1002/9781118584248.ch5">https://doi.org/10.1002/9781118584248.ch5</a>
</div>
<div id="ref-podesvakajino2014" class="csl-entry" role="listitem">
Podesva, R. J., &amp; Kajino, S. (2014b). Sociophonetics, gender, and sexuality. <em>The Handbook of Language, Gender, and Sexuality</em>, 103–122.
</div>
<div id="ref-repp1982" class="csl-entry" role="listitem">
Repp, B. H. (1982). Phonetic trading relations and context effects: New experimental evidence for a speech mode of perception. <em>Psychological Bulletin</em>, <em>92</em>(1), 81.
</div>
<div id="ref-rosseelGrondelaers2019" class="csl-entry" role="listitem">
Rosseel, L., &amp; Grondelaers, S. (2019). Implicitness and experimental methods in language variation research. <em>Linguistics Vanguard</em>, <em>5</em>(s1).
</div>
<div id="ref-rubin1992" class="csl-entry" role="listitem">
Rubin, D. L. (1992). Nonlanguage factors affecting undergraduates’ judgments of nonnative english-speaking teaching assistants. <em>Research in Higher Education</em>, <em>33</em>(4), 511–531.
</div>
<div id="ref-samolinski2007" class="csl-entry" role="listitem">
Samoliński, B. K., Grzanka, A., &amp; Gotlib, T. (2007). Changes in nasal cavity dimensions in children and adults by gender and age. <em>The Laryngoscope</em>, <em>117</em>(8), 1429–1433.
</div>
<div id="ref-schellingerMunsonEdwards2017" class="csl-entry" role="listitem">
Schellinger, S. K., Munson, B., &amp; Edwards, J. (2017). Gradient perception of children’s productions of/s/and/<span class="math inline">\theta</span>: A comparative study of rating methods. <em>Clinical Linguistics &amp; Phonetics</em>, <em>31</em>(1), 80–103.
</div>
<div id="ref-shadle1991" class="csl-entry" role="listitem">
Shadle, C. H. (1991). The effect of geometry on source mechanisms of fricative consonants. <em>Journal of Phonetics</em>, <em>19</em>(3-4), 409–424.
</div>
<div id="ref-steckerDOnofrioVolume" class="csl-entry" role="listitem">
Stecker, A., &amp; D’Onofrio, A. (this volume). Recognizing uptalk: Memory and metalinguistic commentary for a sociolinguistic feature. <em>Journal of Sociolinguistics</em>, <em>42</em>(1).
</div>
<div id="ref-strand1999" class="csl-entry" role="listitem">
Strand, E. A. (1999). Uncovering the role of gender stereotypes in speech perception. <em>Journal of Language and Social Psychology</em>, <em>18</em>(1), 86–100.
</div>
<div id="ref-strandJohnson1996" class="csl-entry" role="listitem">
Strand, E. A., &amp; Johnson, K. (1996). Gradient and visual speaker normalization in the perception of fricatives. <em>KONVENS</em>, 14–26.
</div>
<div id="ref-trippMunson2022" class="csl-entry" role="listitem">
Tripp, A., &amp; Munson, B. (2022). Perceiving gender while perceiving language: Integrating psycholinguistics and gender theory. <em>Wiley Interdisciplinary Reviews: Cognitive Science</em>, <em>13</em>(2), e1583.
</div>
<div id="ref-walkerHay2011" class="csl-entry" role="listitem">
Walker, A., &amp; Hay, J. (2011). Congruence between ‘word age’and ‘voice age’facilitates lexical access. <em>Laboratory Phonology</em>, <em>2</em>(1).
</div>
<div id="ref-whalen1981" class="csl-entry" role="listitem">
Whalen, D. H. (1981). Effects of vocalic formant transitions and vowel quality on the english [s]–[<span>š</span>] boundary. <em>The Journal of the Acoustical Society of America</em>, <em>69</em>(1), 275–282.
</div>
<div id="ref-whalen1984" class="csl-entry" role="listitem">
Whalen, D. H. (1984). Subcategorical phonetic mismatches slow phonetic judgments. <em>Perception &amp; <span>Psychophysics</span></em>, <em>35</em>, 49–64.
</div>
<div id="ref-whalen1991" class="csl-entry" role="listitem">
Whalen, D. H. (1991). Perception of the english/s/–//distinction relies on fricative noises and transitions, not on brief spectral slices. <em>The Journal of the Acoustical Society of America</em>, <em>90</em>(4), 1776–1785.
</div>
<div id="ref-zimman2017" class="csl-entry" role="listitem">
Zimman, L. (2017). Gender as stylistic bricolage: Transmasculine voices and the relationship between fundamental frequency and/s. <em>Language in Society</em>, <em>46</em>(3), 339–370.
</div>
</div>
<div id="fig-stimuli" class="quarto-float quarto-figure quarto-figure-center FigureWithoutNote" data-fignum="1" prefix="" data-custom-style="FigureWithoutNote">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-stimuli-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;1</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p><span class="citation" data-cites="bouavichithEtAl2019">Bouavichith et al. (<a href="#ref-bouavichithEtAl2019" role="doc-biblioref">2019</a>)</span> auditory stimulus continua. S1, S2, S3, S4, and S5 represent continuum steps from most <em>sack</em>-like to most <em>shack</em>-like fricatives. F0 and F1:F2 Ratio plots show the manipulations to the Male and Female voiced vowels.</p>
</div>
</figcaption>
<div aria-describedby="fig-stimuli-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/figure1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: @bouavichithEtAl2019 auditory stimulus continua. S1, S2, S3, S4, and S5 represent continuum steps from most sack-like to most shack-like fricatives. F0 and F1:F2 Ratio plots show the manipulations to the Male and Female voiced vowels."><img src="images/figure1.png" class="img-fluid figure-img"></a>
</div>
</figure>
</div>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>We are choosing the words ‘congruous’ and ‘incongruous’ intentionally to suggest faces and voices may pattern together in particular ways in listeners’ experience and perception with no implied claim that voices may ‘match’ or ‘mismatch’ in some way that suggests either experimenters or participants have veridical access to an objective reality<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>We are choosing the words ‘congruous’ and ‘incongruous’ intentionally to suggest faces and voices may pattern together in particular ways in listeners’ experience and perception with no implied claim that voices may ‘match’ or ‘mismatch’ in some way that suggests either experimenters or participants have veridical access to an objective reality<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{laycock2024,
  author = {Laycock, Kyler and B McGowan, \&nbsp;Kevin},
  title = {Removing the Disguise: The Matched Guise Technique and
    Listener Awareness},
  date = {2024-10-18},
  langid = {en},
  abstract = {Sociophonetic perception is often studied using versions
    of the matched guise technique. Linguists using this technique
    appear united in the methodological assumptions that participants
    believe the manipulation and that this belief influences perception
    below the level of introspective awareness. We report an audiovisual
    matched guise experiment with a novel “unhidden” instruction
    condition. The basic task is a replication of the Strand effect
    {[}@strandJohnson1996; @strand1999{]}. Participants in the
    “unhidden” condition were instructed that the man or woman in the
    photo did not represent the voice they were listening to.
    Participants in both guises exhibited the Strand effect to nearly
    numerically identical extents. This result suggests that
    participants need not believe a link exists between a voice and a
    purported social category for visually-cued social information to
    influence segmental perception. We explore the implications of this
    result for the MGT and for theories of social awareness and speech
    perception more broadly.}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-laycock2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Laycock, K., &amp; B McGowan, &amp;nbsp;Kevin. (2024, October 18).
<em>Removing the disguise: the matched guise technique and listener
awareness</em>. Awareness and Control of Sociolinguistic Variation.
</div></div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="kbmcgowan/play-manuscript" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->

<div id="criticnav">
<ul>
<li id="markup-button">Markup</li>
<li id="original-button">Original</li>
<li id="edited-button">Edited</li>
</ul>
</div>

<script type="text/javascript">
  function critic() {

      $('.content').addClass('markup');
      $('#markup-button').addClass('active');
      $('ins.break').unwrap();
      $('span.critic.comment').wrap('<span class="popoverc" /></span>');
      $('span.critic.comment').before('&#8225;');
  }

  function original() {
      $('#original-button').addClass('active');
      $('#edited-button').removeClass('active');
      $('#markup-button').removeClass('active');

      $('.content').addClass('original');
      $('.content').removeClass('edited');
      $('.content').removeClass('markup');
  }

  function edited() {
      $('#original-button').removeClass('active');
      $('#edited-button').addClass('active');
      $('#markup-button').removeClass('active');

      $('.content').removeClass('original');
      $('.content').addClass('edited');
      $('.content').removeClass('markup');
  } 

  function markup() {
      $('#original-button').removeClass('active');
      $('#edited-button').removeClass('active');
      $('#markup-button').addClass('active');

      $('.content').removeClass('original');
      $('.content').removeClass('edited');
      $('.content').addClass('markup');
  }

  var o = document.getElementById("original-button");
  var e = document.getElementById("edited-button");
  var m = document.getElementById("markup-button");

  window.onload = critic();
  o.onclick = original;
  e.onclick = edited;
  m.onclick = markup;
</script>
<script>var lightboxQuarto = GLightbox({"selector":".lightbox","closeEffect":"zoom","loop":false,"descPosition":"bottom","openEffect":"zoom"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>